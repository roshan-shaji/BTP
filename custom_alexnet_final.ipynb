{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07EzBGbGQRw_"
   },
   "source": [
    "This notebook has custom AlexNet. It measures:\n",
    "\n",
    "\n",
    "1.   Sparsity of weights(one-time)\n",
    "2.   Layerwise CONV layer activation sparsities\n",
    "3.   Accuracy of the model\n",
    "4. Layerwise #MAC ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQAEW9fKRDnL"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jQ8KMus_QpKG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO0iFxRTRHwv"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfnuijOhRFc7",
    "outputId": "3db16282-4c34-4bf7-b0d0-1528ba1dd819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(data_dir,\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=train_transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=valid_transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "def get_test_loader(data_dir,\n",
    "                    batch_size,\n",
    "                    shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    # define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227,227)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir = './data', batch_size = 1, augment = False, random_seed = 1)\n",
    "test_loader = get_test_loader(data_dir = './data', batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bwod9_BShbO"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_0ZHVsbSjgR"
   },
   "source": [
    "**Custom conv2d function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RROzmY9uR_bY"
   },
   "outputs": [],
   "source": [
    "def myconv2d(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    \"\"\"\n",
    "    Function to process an input with a standard convolution\n",
    "    \"\"\"\n",
    "    mul_count = 0\n",
    "    # print('input', input.shape)\n",
    "    # print('wt', weight.shape)\n",
    "    batch_size, in_channels, in_h, in_w = input.shape\n",
    "    out_channels, in_channels, kh, kw = weight.shape\n",
    "    out_h = int((in_h - kh + 2 * padding[0]) / stride[0] + 1)\n",
    "    out_w = int((in_w - kw + 2 * padding[1]) / stride[1] + 1)\n",
    "    unfold = torch.nn.Unfold(kernel_size=(kh, kw), dilation=dilation, padding=padding, stride=stride)\n",
    "    inp_unf = unfold(input)\n",
    "    w_ = weight.view(weight.size(0), -1).t()\n",
    "    if bias is None:\n",
    "        out_unf = inp_unf.transpose(1, 2).matmul(w_).transpose(1, 2)\n",
    "        mul_count += 1\n",
    "    else:\n",
    "        out_unf = (inp_unf.transpose(1, 2).matmul(w_) + bias).transpose(1, 2)\n",
    "        mul_count += 1\n",
    "    out = out_unf.view(batch_size, out_channels, out_h, out_w)\n",
    "    return (out.float(), mul_count)\n",
    "    # return out.float()\n",
    "    \n",
    "###################################################################################################### \n",
    "\n",
    "class comp_vector():\n",
    "  def __init__(self, arr):\n",
    "    self.x = arr.size(dim=2)\n",
    "    self.y = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.index_vector = []\n",
    "    self.data_vector = []\n",
    "    for i in range(self.c):\n",
    "      # print(arr[i])\n",
    "      self.index_vector.append(np.flatnonzero(arr[i].cpu()))\n",
    "      self.data_vector.append(arr[i].ravel()[self.index_vector[-1]])\n",
    "\n",
    "    # index_vector = np.flatnonzero(arr)\n",
    "    # data_vector = arr.ravel()[index_vector]\n",
    "\n",
    "  def get_index_vector(self):\n",
    "    return self.index_vector\n",
    "\n",
    "  def get_data_vector(self):\n",
    "    return self.data_vector\n",
    "\n",
    "\n",
    "def conv_compressed(comp_inp, comp_wt):\n",
    "    acc_x, acc_y, acc_c = int((comp_inp.x - comp_wt.x)//stride  + 1) , int((comp_inp.y - comp_wt.y)//stride  +1), comp_wt.c\n",
    "#     print(acc_x, acc_y, acc_c)\n",
    "    mult_count = 0\n",
    "    # print(acc_x, acc_y, acc_c)\n",
    "    acc_buf = torch.FloatTensor(acc_x, acc_y).zero_()\n",
    "    inp_index_vector = comp_inp.get_index_vector()\n",
    "    inp_data_vector = comp_inp.get_data_vector()\n",
    "    wt_index_vector = comp_wt.get_index_vector()\n",
    "    wt_data_vector = comp_wt.get_data_vector()\n",
    "    # print(inp_index_vector[0])\n",
    "    # print(len(inp_index_vector[0]))\n",
    "    for c in range(acc_c):\n",
    "      for i in range(len(inp_index_vector[c])):\n",
    "        for j in range(len(wt_index_vector[c])):\n",
    "          inp_x = inp_index_vector[c][i]//comp_inp.x\n",
    "          inp_y = inp_index_vector[c][i]%comp_inp.y\n",
    "          wt_x = wt_index_vector[c][j]//comp_wt.x\n",
    "          wt_y = wt_index_vector[c][j]%comp_wt.y\n",
    "\n",
    "          out_x = (inp_x - wt_x)\n",
    "          out_y = (inp_y- wt_y)\n",
    "          if out_x%stride==0 and out_y%stride==0:\n",
    "            out_x = out_x//stride\n",
    "            out_y = out_y//stride\n",
    "            # print(out_x, out_y,c,i,j,)\n",
    "            if 0<=out_x<acc_x and 0<=out_y<acc_y:\n",
    "              # print(\"yes\")\n",
    "              acc_buf[out_x][out_y]+=float(inp_data_vector[c][i] * wt_data_vector[c][j])\n",
    "              mult_count +=1\n",
    "    \n",
    "    return (acc_buf,mult_count)\n",
    "\n",
    "def myconv2d_sparse(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "  print(input.size())\n",
    "  comp_in = comp_vector(input[0])\n",
    "  in_x = input.size(dim=3)\n",
    "  in_y = input.size(dim=2)\n",
    "  wt_x = weight.size(dim=3)\n",
    "  wt_y = weight.size(dim=2)\n",
    "  c = weight.size(dim=1)\n",
    "  k = weight.size(dim=0)\n",
    "  out = torch.empty(size=(1,k, int((in_x-wt_x)/stride[0]+1), int((in_y-wt_y)/stride[1]+1)))\n",
    "\n",
    "  mult_count = 0\n",
    "  for i in range(k):\n",
    "    comp_wt = comp_vector(weight[i])\n",
    "    out[0][i], num =conv_compressed(comp_in, comp_wt, stride[0])\n",
    "    mult_count+=num\n",
    "  \n",
    "  return (out,mult_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkUJl1GPSz7u"
   },
   "source": [
    "**Defining custom Conv2D Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6duNFz6pSsuR"
   },
   "outputs": [],
   "source": [
    "class Custom_Conv2d(torch.nn.modules.conv._ConvNd):\n",
    "    \"\"\"\n",
    "    Implements a standard convolution layer that can be used as a regular module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        stride = (stride, stride)\n",
    "        padding = (padding, padding)\n",
    "        dilation = (dilation, dilation)\n",
    "        super(Custom_Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, (0, 0), groups, bias, padding_mode)\n",
    "\n",
    "    def conv2d_forward(self, input, weight):\n",
    "        return myconv2d_sparse(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv2d_forward(input, self.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSoVmFiKTGMx"
   },
   "source": [
    "**Defining custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5vU3IzS1srIo"
   },
   "outputs": [],
   "source": [
    "# empty arrays for storing activation sparsities\n",
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "c4 = []\n",
    "c5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kIg17oiX0-nc"
   },
   "outputs": [],
   "source": [
    "# empty arrays for storing #MACops per layer\n",
    "m1 = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "m5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5DIHcqOOTBKe"
   },
   "outputs": [],
   "source": [
    "class CustomAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            Custom_Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "            Custom_Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "            Custom_Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            Custom_Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            Custom_Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print('Sparsity of CONV1 activations: ', (1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "        c1.append((1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "        out, macops1 = self.features[0](x)             \n",
    "        m1.append(macops1)\n",
    "        out = self.features[1](out)\n",
    "        out = self.features[2](out)\n",
    "        \n",
    "        # print('Sparsity of CONV2 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c2.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops2 = self.features[3](out)\n",
    "        m2.append(macops2)\n",
    "        out = self.features[4](out)\n",
    "        out = self.features[5](out)\n",
    "        \n",
    "        # print('Sparsity of CONV3 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c3.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops3 = self.features[6](out)\n",
    "        m3.append(macops3)\n",
    "        out = self.features[7](out)\n",
    "        \n",
    "        # print('Sparsity of CONV4 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c4.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops4 = self.features[8](out)\n",
    "        m4.append(macops4)\n",
    "        out = self.features[9](out)\n",
    "        \n",
    "        c5.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        # print('Sparsity of CONV5 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops5 = self.features[10](out)\n",
    "        m5.append(macops5)\n",
    "        out = self.features[11](out)\n",
    "        out = self.features[12](out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3s82aEvUACf"
   },
   "source": [
    "\n",
    "\n",
    "**Instantiating a custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xaR1XvzgT7cK"
   },
   "outputs": [],
   "source": [
    "cust_alexnet = CustomAlexNet(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpwa7XQFUiCU"
   },
   "source": [
    "**Loading pretrained AlexNet weights into our custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "FDY_ZerKUJwk",
    "outputId": "8403f8c4-8fc8-4c59-d474-17ab6337055c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomAlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Custom_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Custom_Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Custom_Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Custom_Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Custom_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alexnet = torch.load(r'alexnet.pth')\n",
    "cust_alexnet.load_state_dict(torch.load(r'alexnet.pth'))\n",
    "cust_alexnet.to(device)\n",
    "cust_alexnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNXnVogrcvjr"
   },
   "source": [
    "# Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfZGMh9Ccx4X"
   },
   "source": [
    "**Accuracy & Layer-wise Activation Sparsities(Dynamic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VpctHowbcIWn"
   },
   "outputs": [],
   "source": [
    "def test_model(cust_alexnet):\n",
    "  cust_alexnet.to(device)\n",
    "  cust_alexnet.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  m1.clear()\n",
    "  m2.clear()\n",
    "  m3.clear()\n",
    "  m4.clear()\n",
    "  m5.clear()\n",
    "  c1.clear()\n",
    "  c2.clear()\n",
    "  c3.clear()\n",
    "  c4.clear()\n",
    "  c5.clear()\n",
    "  with torch.no_grad():\n",
    "      for data in test_loader:\n",
    "          if(total>=1):\n",
    "            break\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          outputs = cust_alexnet(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the 10000 test images: %.2f %%' % (100 * correct / total))\n",
    "  print(\"CONV1 #MACops(avg):\", sum(m1)/len(m1))\n",
    "  print(\"CONV2 #MACops(avg):\", sum(m2)/len(m2))\n",
    "  print(\"CONV3 #MACops(avg):\", sum(m3)/len(m3))\n",
    "  print(\"CONV4 #MACops(avg):\", sum(m4)/len(m4))\n",
    "  print(\"CONV5 #MACops(avg):\", sum(m5)/len(m5))\n",
    "\n",
    "  print(\"CONV1 activation sparsity(avg):\", sum(c1)/len(c1))\n",
    "  print(\"CONV2 activation sparsity(avg):\", sum(c2)/len(c2))\n",
    "  print(\"CONV3 activation sparsity(avg):\", sum(c3)/len(c3))\n",
    "  print(\"CONV4 activation sparsity(avg):\", sum(c4)/len(c4))\n",
    "  print(\"CONV5 activation sparsity(avg):\", sum(c5)/len(c5))\n",
    "  # sum(c2)/len(c2)\n",
    "  print(len(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnCjp50SwvJ_",
    "outputId": "de7976d8-86f6-47f6-d428-a594fad6d81a"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4924/4014264336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcust_alexnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4924/2651501855.py\u001b[0m in \u001b[0;36mtest_model\u001b[1;34m(cust_alexnet)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m           \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcust_alexnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m           \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m           \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4924/1705018328.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# print('Sparsity of CONV1 activations: ', (1 - torch.count_nonzero(x)/torch.numel(x)).item())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mc1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmacops1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmacops1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4924/2704380635.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4924/2704380635.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         return myconv2d_sparse(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m     18\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4924/1616850679.py\u001b[0m in \u001b[0;36mmyconv2d_sparse\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     90\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mcomp_wt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mconv_compressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomp_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomp_wt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[0mmult_count\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4924/1616850679.py\u001b[0m in \u001b[0;36mconv_compressed\u001b[1;34m(comp_inp, comp_wt)\u001b[0m\n\u001b[0;32m     72\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mout_x\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0macc_x\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mout_y\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0macc_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;31m# print(\"yes\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0macc_buf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_x\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_data_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mwt_data_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mmult_count\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(cust_alexnet)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx7IdeTXc5ag"
   },
   "source": [
    "**Weight Sparsity (static)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "stSC9uncVI9r"
   },
   "outputs": [],
   "source": [
    "def get_alex_w_sparsities(cust_alexnet):\n",
    "  conv_indices = [0, 3, 6, 8, 10]\n",
    "\n",
    "  for i in range(5):\n",
    "    layer_index = conv_indices[i]\n",
    "\n",
    "    print(\n",
    "        \"Sparsity in conv{:}.weight: {:.2f}%\".format(i+1, \n",
    "            100. * float(torch.sum(cust_alexnet.features[layer_index].weight == 0))\n",
    "            / float(cust_alexnet.features[layer_index].weight.nelement())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ushoqca7d5s5",
    "outputId": "4c9c1a2d-3cc1-4923-9085-803c046abb6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 0.00%\n",
      "Sparsity in conv2.weight: 0.00%\n",
      "Sparsity in conv3.weight: 0.00%\n",
      "Sparsity in conv4.weight: 0.00%\n",
      "Sparsity in conv5.weight: 0.00%\n"
     ]
    }
   ],
   "source": [
    "get_alex_w_sparsities(cust_alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5w4M9T6yS_K"
   },
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percs = [.25, .50, .75, ]\n",
    "# new_model = LeNet()\n",
    "# for name, module in new_model.named_modules():\n",
    "#     # prune 20% of connections in all 2D-conv layers\n",
    "#     if isinstance(module, torch.nn.Conv2d):\n",
    "#         prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "#     # prune 40% of connections in all linear layers\n",
    "#     elif isinstance(module, torch.nn.Linear):\n",
    "#         prune.l1_unstructured(module, name='weight', amount=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomAlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Custom_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Custom_Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Custom_Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Custom_Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Custom_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_90 = CustomAlexNet(10)\n",
    "alex_90.load_state_dict(torch.load(r'alexnet.pth'))\n",
    "\n",
    "for name, module in alex_90.named_modules():\n",
    "    # prune 90% of connections in all 2D-conv layers\n",
    "    if isinstance(module, Custom_Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 25.00%\n",
      "Sparsity in conv2.weight: 25.00%\n",
      "Sparsity in conv3.weight: 25.00%\n",
      "Sparsity in conv4.weight: 25.00%\n",
      "Sparsity in conv5.weight: 25.00%\n"
     ]
    }
   ],
   "source": [
    "get_alex_w_sparsities(alex_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 83.35 %\n",
      "CONV1 #MACops(avg): 1.0\n",
      "CONV2 #MACops(avg): 1.0\n",
      "CONV3 #MACops(avg): 1.0\n",
      "CONV4 #MACops(avg): 1.0\n",
      "CONV5 #MACops(avg): 1.0\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.4914952580094337\n",
      "CONV3 activation sparsity(avg): 0.7596199704170227\n",
      "CONV4 activation sparsity(avg): 0.8848602611422539\n",
      "CONV5 activation sparsity(avg): 0.8694982788264751\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "test_model(alex_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomAlexNet(10)\n",
    "model.load_state_dict(torch.load(r'alexnet.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[3], 'weight'),\n",
    "    (model.features[6], 'weight'),\n",
    "    (model.features[8], 'weight'),\n",
    "    (model.features[10], 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 9.78%\n",
      "Sparsity in conv2.weight: 24.24%\n",
      "Sparsity in conv3.weight: 23.41%\n",
      "Sparsity in conv4.weight: 26.49%\n",
      "Sparsity in conv5.weight: 25.55%\n"
     ]
    }
   ],
   "source": [
    "get_alex_w_sparsities(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 83.46 %\n",
      "CONV1 #MACops(avg): 1.0\n",
      "CONV2 #MACops(avg): 1.0\n",
      "CONV3 #MACops(avg): 1.0\n",
      "CONV4 #MACops(avg): 1.0\n",
      "CONV5 #MACops(avg): 1.0\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.4925482948899269\n",
      "CONV3 activation sparsity(avg): 0.7564612115383148\n",
      "CONV4 activation sparsity(avg): 0.8869362964272499\n",
      "CONV5 activation sparsity(avg): 0.8687332128226757\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
