{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otCAVT_liSg3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JxoP7NViHRP"
      },
      "outputs": [],
      "source": [
        "# ALGO1: nn.conv2d\n",
        "def myconv2d(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
        "    \"\"\"\n",
        "    Function to process an input with a standard convolution\n",
        "    \"\"\"\n",
        "    mul_count = 0\n",
        "#     print('input', input.shape)\n",
        "#     print('wt', weight.shape)\n",
        "    batch_size, in_channels, in_h, in_w = input.shape\n",
        "    out_channels, in_channels, kh, kw = weight.shape\n",
        "    out_h = int((in_h - kh + 2 * padding[0]) / stride[0] + 1)\n",
        "    out_w = int((in_w - kw + 2 * padding[1]) / stride[1] + 1)\n",
        "    unfold = torch.nn.Unfold(kernel_size=(kh, kw), dilation=dilation, padding=padding, stride=stride)\n",
        "    inp_unf = unfold(input)\n",
        "    w_ = weight.view(weight.size(0), -1).t()\n",
        "    if bias is None:\n",
        "        out_unf = inp_unf.transpose(1, 2).matmul(w_).transpose(1, 2)\n",
        "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
        "    else:\n",
        "        out_unf = (inp_unf.transpose(1, 2).matmul(w_) + bias).transpose(1, 2)\n",
        "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
        "    out = out_unf.view(batch_size, out_channels, out_h, out_w)\n",
        "#     print(out)\n",
        "    return (out.float(), mul_count)\n",
        "    # return out.float()\n",
        "\n",
        "##############################################################################################\n",
        "\n",
        "class comp_vector():\n",
        "  def __init__(self, arr):\n",
        "    self.x = arr.size(dim=2)\n",
        "    self.y = arr.size(dim=1)\n",
        "    self.c = arr.size(dim=0)\n",
        "    self.index_vector = []\n",
        "    self.data_vector = []\n",
        "    for i in range(self.c):\n",
        "      # print(arr[i])\n",
        "      self.index_vector.append(np.flatnonzero(arr[i].cpu()))\n",
        "      self.data_vector.append(arr[i].ravel()[self.index_vector[-1]])\n",
        "\n",
        "    # index_vector = np.flatnonzero(arr)\n",
        "    # data_vector = arr.ravel()[index_vector]\n",
        "\n",
        "  def get_index_vector(self):\n",
        "    return self.index_vector\n",
        "\n",
        "  def get_data_vector(self):\n",
        "    return self.data_vector\n",
        "\n",
        "\n",
        "def conv_compressed(comp_inp, comp_wt, stride=1):\n",
        "#     print('called conv_compressed')\n",
        "    acc_x, acc_y, acc_c = int((comp_inp.x - comp_wt.x)//stride  + 1) , int((comp_inp.y - comp_wt.y)//stride  +1), comp_wt.c\n",
        "#     print(acc_x, acc_y, acc_c)\n",
        "    mult_count = 0\n",
        "    # print(acc_x, acc_y, acc_c)\n",
        "    acc_buf = torch.FloatTensor(acc_x, acc_y).zero_()\n",
        "    inp_index_vector = comp_inp.get_index_vector()\n",
        "    inp_data_vector = comp_inp.get_data_vector()\n",
        "    wt_index_vector = comp_wt.get_index_vector()\n",
        "    wt_data_vector = comp_wt.get_data_vector()\n",
        "    # print(inp_index_vector[0])\n",
        "    # print(len(inp_index_vector[0]))\n",
        "    for c in range(acc_c):\n",
        "      for i in range(len(inp_index_vector[c])):\n",
        "        for j in range(len(wt_index_vector[c])):\n",
        "          inp_x = inp_index_vector[c][i]//comp_inp.x\n",
        "          inp_y = inp_index_vector[c][i]%comp_inp.y\n",
        "          wt_x = wt_index_vector[c][j]//comp_wt.x\n",
        "          wt_y = wt_index_vector[c][j]%comp_wt.y\n",
        "\n",
        "          out_x = (inp_x - wt_x)\n",
        "          out_y = (inp_y- wt_y)\n",
        "          if out_x%stride==0 and out_y%stride==0:\n",
        "            out_x = out_x//stride\n",
        "            out_y = out_y//stride\n",
        "            # print(out_x, out_y,c,i,j,)\n",
        "            if 0<=out_x<acc_x and 0<=out_y<acc_y:\n",
        "              # print(\"yes\")\n",
        "              acc_buf[out_x][out_y]+=float(inp_data_vector[c][i] * wt_data_vector[c][j])\n",
        "              mult_count +=1\n",
        "    \n",
        "    return (acc_buf,mult_count)\n",
        "\n",
        "def myconv2d_sparse(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
        "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
        "#   print(input.size())\n",
        "  comp_in = comp_vector(input[0])\n",
        "  in_x = input.size(dim=3)\n",
        "  in_y = input.size(dim=2)\n",
        "  wt_x = weight.size(dim=3)\n",
        "  wt_y = weight.size(dim=2)\n",
        "  c = weight.size(dim=1)\n",
        "  k = weight.size(dim=0)\n",
        "  out = torch.empty(size=(1,k, int((in_x-wt_x)/stride[0]+1), int((in_y-wt_y)/stride[1]+1)))\n",
        "\n",
        "  mult_count = 0\n",
        "  for i in range(k):\n",
        "    comp_wt = comp_vector(weight[i])\n",
        "    out[0][i], num =conv_compressed(comp_in, comp_wt, stride[0])\n",
        "    out[0][i] += bias[i]\n",
        "    mult_count+=num\n",
        "#   print(out)\n",
        "  return (out,mult_count)\n",
        "\n",
        "######################################################################################################\n",
        "\n",
        "# ALGO 3\n",
        "def compute_weight_list(kernel):    \n",
        "    kernels = []\n",
        "    filter_count = kernel.shape[0]\n",
        "    depth = kernel.shape[1]\n",
        "    height = kernel.shape[2]\n",
        "    width = kernel.shape[3]\n",
        "    for f in range(filter_count):\n",
        "        weight_list = []\n",
        "        for k in range(depth):\n",
        "            for i in range(height):\n",
        "                for j in range(width):\n",
        "                    w = kernel[f][k][i][j]\n",
        "                    if w < 0:\n",
        "                        weight_list.append(tuple((w, k, i, j)))\n",
        "        sorted_weight_list = sorted(weight_list, key = lambda x: x[0])\n",
        "        kernels.append(sorted_weight_list)\n",
        "    return kernels\n",
        "\n",
        "def compute_conv_onlypred(img, weight_list, weights, r, c, bias=0):\n",
        "    img_out_cell = 0\n",
        "    conv_mult_count = 0\n",
        "    depth = weights.shape[0]\n",
        "    height = weights.shape[1]\n",
        "    width = weights.shape[2]\n",
        "    for k in range(depth):\n",
        "        for i in range(width):\n",
        "            for j in range(height):          \n",
        "                if weights[k][i][j]>0:\n",
        "                  #  and r+i<img.shape[2] and c+j<img.shape[3]\n",
        "                    conv_mult_count += 1 \n",
        "                    img_out_cell += img[0][k][r+i][c+j]*weights[k][i][j]\n",
        "\n",
        "    img_out_cell+=bias\n",
        "    \n",
        "    for tup in weight_list:\n",
        "        conv_mult_count += 1\n",
        "        # if r+tup[2]>=img.shape[2] or c+tup[3]>=img.shape[3]:\n",
        "        #   continue\n",
        "        img_out_cell += tup[0]*img[0][tup[1]][r+tup[2]][c+tup[3]]\n",
        "        if img_out_cell < 0:\n",
        "            break\n",
        "    return img_out_cell, conv_mult_count\n",
        "\n",
        "def compute_filter_conv_onlypred(img, weight_list, weights, kernel_id,stride=(1,1), padding=(0,0), bias=0):\n",
        "    width_out = int((img.shape[3]-weights.shape[2])/stride[1]+1)\n",
        "    height_out = int((img.shape[2]-weights.shape[1])/stride[0]+1)\n",
        "    img_out_channel = torch.zeros(width_out,height_out)\n",
        "    filter_mult_count = 0\n",
        "    # print(img.shape[2]+2*padding[0]-weights.shape[1], img.shape[3]+2*padding[1]-weights.shape[2])\n",
        "    for r in range(0,img.shape[2]-weights.shape[1]+1,stride[0]):\n",
        "        for c in range(0,img.shape[3]-weights.shape[2]+1,stride[1]):\n",
        "            r_out = int(r/stride[0])\n",
        "            c_out = int(c/stride[1])\n",
        "            # print(r_out, c_out)\n",
        "            img_out_channel[r_out][c_out], mult_count = compute_conv_onlypred(img, weight_list, weights, r, c, bias)\n",
        "            # img_out_channel[r_out][c_out] += bias\n",
        "            filter_mult_count += mult_count\n",
        "    return img_out_channel, filter_mult_count\n",
        "\n",
        "def myconv2d_onlypred(img, weights, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
        "    img = torch.nn.functional.pad(img, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
        "    layer_mult_count = 0\n",
        "    filter_count = weights.shape[0]\n",
        "    depth = weights.shape[1]\n",
        "    height = weights.shape[2]\n",
        "    width = weights.shape[3]\n",
        "    channels_out=filter_count\n",
        "    width_out = int((img.shape[3]-width)/stride[1]+1)\n",
        "    height_out = int((img.shape[2]-height)/stride[0]+1)\n",
        "    img_conv_output = torch.zeros(1, channels_out, width_out, height_out)\n",
        "    filters_list = compute_weight_list(weights)\n",
        "    for kernel_id in range(filter_count):\n",
        "        if kernel_id%8==0:\n",
        "            print(\"kernel_id\", kernel_id)\n",
        "        weight_list = filters_list[kernel_id]\n",
        "        img_conv_channel, mult_count = compute_filter_conv_onlypred(img, weight_list, weights[kernel_id], kernel_id, stride, padding, bias[kernel_id])\n",
        "        img_conv_output[0][kernel_id] = img_conv_channel\n",
        "        layer_mult_count += mult_count\n",
        "    return (img_conv_output, layer_mult_count)\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "# ALGO 4\n",
        "class comp_vector_pred():\n",
        "  def __init__(self, arr):\n",
        "    self.y = arr.size(dim=2)\n",
        "    self.x = arr.size(dim=1)\n",
        "    self.c = arr.size(dim=0)\n",
        "    self.pos_vector = [] #stores tuples of (data, index)\n",
        "    self.neg_vector = []\n",
        "\n",
        "    for k in range(self.c):\n",
        "            for i in range(self.x):\n",
        "                for j in range(self.y):\n",
        "                    w = arr[k][i][j]\n",
        "                    if w > 0:\n",
        "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
        "                    elif w<0:\n",
        "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
        "\n",
        "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
        "\n",
        "  def get_pos_vector(self):\n",
        "    return self.pos_vector\n",
        "\n",
        "  def get_neg_vector(self):\n",
        "    return self.neg_vector\n",
        "\n",
        "\n",
        "def compute_conv_sparsepred(input, weight, comp_wt, r, c, bias=0):\n",
        "  img_out_cell = 0\n",
        "  conv_mult_count = 0\n",
        "  pos = comp_wt.get_pos_vector()\n",
        "  neg = comp_wt.get_neg_vector()\n",
        "\n",
        "  x = weight.shape[1]\n",
        "  y = weight.shape[2]\n",
        "  k = weight.shape[0]\n",
        "\n",
        "  mult_nonzero = 0\n",
        "  for channel in range(k):\n",
        "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
        "    inp_nonzero = np.flatnonzero(inp_window)\n",
        "    wt_nonzero = np.flatnonzero(weight[channel])\n",
        "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
        "    mult_nonzero += common\n",
        "\n",
        "  for tup in pos:\n",
        "    # if(r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]):\n",
        "    #   continue\n",
        "    conv_mult_count += 1\n",
        "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
        "\n",
        "  img_out_cell+=bias\n",
        "\n",
        "  for tup in neg:\n",
        "    if img_out_cell < 0:\n",
        "      break\n",
        "    # if(r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]):\n",
        "    #   continue\n",
        "    conv_mult_count += 1\n",
        "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
        "\n",
        "  return img_out_cell, conv_mult_count, mult_nonzero\n",
        "\n",
        "\n",
        "def compute_filter_conv_sparsepred(input, weights, comp_wt, width_out, height_out,stride=(1,1), padding=(0,0), bias=0):\n",
        "#     print('called compute_filter_conv')\n",
        "    img_out_channel = torch.zeros(width_out, height_out)\n",
        "    filter_mult_count = 0\n",
        "    filter_calc_mult = 0\n",
        "    for r in range(0,input.shape[2]-weights.shape[1]+1,stride[0]):\n",
        "        for c in range(0,input.shape[3]-weights.shape[2]+1,stride[1]):\n",
        "            r_out = int(r/stride[0])\n",
        "            c_out = int(c/stride[1])\n",
        "            img_out_channel[r_out][c_out], mult_count, calc_mult = compute_conv_sparsepred(input, weights,comp_wt, r, c, bias)\n",
        "            filter_mult_count += mult_count\n",
        "            filter_calc_mult += calc_mult\n",
        "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
        "\n",
        "\n",
        "def myconv2d_sparse_pred(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
        "    input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
        "    in_x = input.shape[2]\n",
        "    in_y = input.shape[3]\n",
        "    wt_x = weight.shape[2]\n",
        "    wt_y = weight.shape[3]\n",
        "    c = weight.shape[1]\n",
        "    filter_count = weight.shape[0]\n",
        "    w = int((in_x-wt_x)//stride[0]+1)\n",
        "    h = int((in_y-wt_y)//stride[1]+1)\n",
        "    out = torch.empty(size=(1, filter_count, w, h))\n",
        "\n",
        "    mult_count = 0\n",
        "    calc_mult = 0\n",
        "    for i in range(filter_count):\n",
        "        comp_wt = comp_vector_pred(weight[i])\n",
        "        out[0][i], num1, num2 =compute_filter_conv_sparsepred(input, weight[i], comp_wt, w, h,stride, padding, bias[i])\n",
        "        # out[0][i] += bias[i]\n",
        "        mult_count+=num1\n",
        "        calc_mult+=num2\n",
        "    # mult_count is predictive sparse(weight only)\n",
        "    # calc_mult is baseline 2(sparse non-predictive)\n",
        "#     print(out)\n",
        "    return (out,mult_count)\n",
        "###################################################################################\n",
        "\n",
        "# ALGO 5\n",
        "class comp_vector_pred_twosided():\n",
        "  def __init__(self, arr):\n",
        "    self.x = arr.size(dim=2)\n",
        "    self.y = arr.size(dim=1)\n",
        "    self.c = arr.size(dim=0)\n",
        "    self.pos_vector = [] #stores tuples of (data, index)\n",
        "    self.neg_vector = []\n",
        "\n",
        "    for k in range(self.c):\n",
        "            for i in range(self.y):\n",
        "                for j in range(self.x):\n",
        "                    w = arr[k][i][j]\n",
        "                    if w > 0:\n",
        "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
        "                    elif w<0:\n",
        "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
        "\n",
        "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
        "\n",
        "  def get_pos_vector(self):\n",
        "    return self.pos_vector\n",
        "\n",
        "  def get_neg_vector(self):\n",
        "    return self.neg_vector\n",
        "\n",
        "\n",
        "def compute_conv_sparsepred_twosided(input, weight, comp_wt, r, c,bias=0):\n",
        "  img_out_cell = 0\n",
        "  conv_mult_count = 0\n",
        "  pos = comp_wt.get_pos_vector()\n",
        "  neg = comp_wt.get_neg_vector()\n",
        "\n",
        "  x = weight.shape[1]\n",
        "  y = weight.shape[2]\n",
        "  k = weight.shape[0]\n",
        "\n",
        "  mult_nonzero = 0\n",
        "  for channel in range(k):\n",
        "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
        "    inp_nonzero = np.flatnonzero(inp_window)\n",
        "    wt_nonzero = np.flatnonzero(weight[channel])\n",
        "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
        "    mult_nonzero += common\n",
        "\n",
        "  for tup in pos:\n",
        "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
        "      #  or r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]\n",
        "      continue\n",
        "    conv_mult_count += 1\n",
        "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
        "\n",
        "  img_out_cell+=bias\n",
        "\n",
        "  # idx = 0\n",
        "  # while img_out_cell>=0 and idx<len(neg):\n",
        "  #   tup = neg[idx]\n",
        "  #   if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
        "  #     continue\n",
        "  #   conv_mult_count += 1\n",
        "  #   img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
        "  #   idx+=1\n",
        "\n",
        "  for tup in neg:\n",
        "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
        "      #  or r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]\n",
        "      continue\n",
        "    if img_out_cell < 0:\n",
        "      break\n",
        "    conv_mult_count += 1\n",
        "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
        "\n",
        "  return img_out_cell, conv_mult_count, mult_nonzero\n",
        "\n",
        "\n",
        "def compute_filter_conv_sparsepred_twosided(input, weights, comp_wt, width_out, height_out,stride=(1,1), padding=(0,0), bias=0):\n",
        "    img_out_channel = torch.zeros(width_out, height_out)\n",
        "    filter_mult_count = 0\n",
        "    filter_calc_mult = 0\n",
        "    # print(input.shape)\n",
        "    # print(weights.shape)\n",
        "    # print(len(stride))\n",
        "    # print(len(padding))\n",
        "    for r in range(0,input.shape[2]-weights.shape[1]+1,stride[0]):\n",
        "        for c in range(0,input.shape[3]-weights.shape[2]+1,stride[1]):\n",
        "            r_out = int(r/stride[0])\n",
        "            c_out = int(c/stride[1])\n",
        "            img_out_channel[r_out][c_out], mult_count, calc_mult = compute_conv_sparsepred_twosided(input, weights,comp_wt, r, c, bias)\n",
        "            # img_out_channel[r][c] += bias \n",
        "            # Bias added in compute_conv_sparsepred_twosided\n",
        "            filter_mult_count += mult_count\n",
        "            filter_calc_mult += calc_mult\n",
        "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
        "\n",
        "\n",
        "def myconv2d_sparse_pred_twosided(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
        "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
        "  in_x = input.shape[2]\n",
        "  in_y = input.shape[3]\n",
        "  wt_x = weight.shape[2]\n",
        "  wt_y = weight.shape[3]\n",
        "  c = weight.shape[1]\n",
        "  filter_count = weight.shape[0]\n",
        "  w = int((in_x-wt_x)//stride[0]+1)\n",
        "  h = int((in_y-wt_y)//stride[1]+1)\n",
        "  # print(w,h)\n",
        "  out = torch.empty(size=(1, filter_count, w, h))\n",
        "\n",
        "  mult_count = 0\n",
        "  calc_mult = 0\n",
        "  for i in range(filter_count):\n",
        "    comp_wt = comp_vector_pred_twosided(weight[i])\n",
        "    out[0][i], num1, num2 =compute_filter_conv_sparsepred_twosided(input, weight[i], comp_wt, w, h,stride=stride,padding=padding,bias=bias[i])\n",
        "    mult_count+=num1\n",
        "    calc_mult+=num2\n",
        "  \n",
        "  return (out,mult_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpEMKGF443GM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
