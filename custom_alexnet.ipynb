{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07EzBGbGQRw_"
   },
   "source": [
    "This notebook has custom AlexNet. It measures:\n",
    "\n",
    "\n",
    "1.   Sparsity of weights(one-time)\n",
    "2.   Layerwise CONV layer activation sparsities\n",
    "3.   Accuracy of the model\n",
    "4. Layerwise #MAC ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQAEW9fKRDnL"
   },
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jQ8KMus_QpKG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "\n",
    "# Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO0iFxRTRHwv"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfnuijOhRFc7",
    "outputId": "3db16282-4c34-4bf7-b0d0-1528ba1dd819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(data_dir,\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=train_transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=valid_transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "def get_test_loader(data_dir,\n",
    "                    batch_size,\n",
    "                    shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    # define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227,227)),\n",
    "        transforms.ToTensor()\n",
    "#         normalize,\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir = './data', batch_size = 1, augment = False, random_seed = 1)\n",
    "test_loader = get_test_loader(data_dir = './data', batch_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bwod9_BShbO"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_0ZHVsbSjgR"
   },
   "source": [
    "**Custom conv2d function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "RROzmY9uR_bY"
   },
   "outputs": [],
   "source": [
    "def myconv2d(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    \"\"\"\n",
    "    Function to process an input with a standard convolution\n",
    "    \"\"\"\n",
    "    mul_count = 0\n",
    "#     print('input', input.shape)\n",
    "#     print('wt', weight.shape)\n",
    "    batch_size, in_channels, in_h, in_w = input.shape\n",
    "    out_channels, in_channels, kh, kw = weight.shape\n",
    "    out_h = int((in_h - kh + 2 * padding[0]) / stride[0] + 1)\n",
    "    out_w = int((in_w - kw + 2 * padding[1]) / stride[1] + 1)\n",
    "    unfold = torch.nn.Unfold(kernel_size=(kh, kw), dilation=dilation, padding=padding, stride=stride)\n",
    "    inp_unf = unfold(input)\n",
    "    w_ = weight.view(weight.size(0), -1).t()\n",
    "    if bias is None:\n",
    "        out_unf = inp_unf.transpose(1, 2).matmul(w_).transpose(1, 2)\n",
    "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
    "    else:\n",
    "        out_unf = (inp_unf.transpose(1, 2).matmul(w_) + bias).transpose(1, 2)\n",
    "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
    "    out = out_unf.view(batch_size, out_channels, out_h, out_w)\n",
    "#     print(out)\n",
    "    return (out.float(), mul_count)\n",
    "    # return out.float()\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "class comp_vector():\n",
    "  def __init__(self, arr):\n",
    "    self.x = arr.size(dim=2)\n",
    "    self.y = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.index_vector = []\n",
    "    self.data_vector = []\n",
    "    for i in range(self.c):\n",
    "      # print(arr[i])\n",
    "      self.index_vector.append(np.flatnonzero(arr[i].cpu()))\n",
    "      self.data_vector.append(arr[i].ravel()[self.index_vector[-1]])\n",
    "\n",
    "    # index_vector = np.flatnonzero(arr)\n",
    "    # data_vector = arr.ravel()[index_vector]\n",
    "\n",
    "  def get_index_vector(self):\n",
    "    return self.index_vector\n",
    "\n",
    "  def get_data_vector(self):\n",
    "    return self.data_vector\n",
    "\n",
    "\n",
    "def conv_compressed(comp_inp, comp_wt, stride=1):\n",
    "#     print('called conv_compressed')\n",
    "    acc_x, acc_y, acc_c = int((comp_inp.x - comp_wt.x)//stride  + 1) , int((comp_inp.y - comp_wt.y)//stride  +1), comp_wt.c\n",
    "#     print(acc_x, acc_y, acc_c)\n",
    "    mult_count = 0\n",
    "    # print(acc_x, acc_y, acc_c)\n",
    "    acc_buf = torch.FloatTensor(acc_x, acc_y).zero_()\n",
    "    inp_index_vector = comp_inp.get_index_vector()\n",
    "    inp_data_vector = comp_inp.get_data_vector()\n",
    "    wt_index_vector = comp_wt.get_index_vector()\n",
    "    wt_data_vector = comp_wt.get_data_vector()\n",
    "    # print(inp_index_vector[0])\n",
    "    # print(len(inp_index_vector[0]))\n",
    "    for c in range(acc_c):\n",
    "      for i in range(len(inp_index_vector[c])):\n",
    "        for j in range(len(wt_index_vector[c])):\n",
    "          inp_x = inp_index_vector[c][i]//comp_inp.x\n",
    "          inp_y = inp_index_vector[c][i]%comp_inp.y\n",
    "          wt_x = wt_index_vector[c][j]//comp_wt.x\n",
    "          wt_y = wt_index_vector[c][j]%comp_wt.y\n",
    "\n",
    "          out_x = (inp_x - wt_x)\n",
    "          out_y = (inp_y- wt_y)\n",
    "          if out_x%stride==0 and out_y%stride==0:\n",
    "            out_x = out_x//stride\n",
    "            out_y = out_y//stride\n",
    "            # print(out_x, out_y,c,i,j,)\n",
    "            if 0<=out_x<acc_x and 0<=out_y<acc_y:\n",
    "              # print(\"yes\")\n",
    "              acc_buf[out_x][out_y]+=float(inp_data_vector[c][i] * wt_data_vector[c][j])\n",
    "              mult_count +=1\n",
    "    \n",
    "    return (acc_buf,mult_count)\n",
    "\n",
    "def myconv2d_sparse(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "#   print(input.size())\n",
    "  comp_in = comp_vector(input[0])\n",
    "  in_x = input.size(dim=3)\n",
    "  in_y = input.size(dim=2)\n",
    "  wt_x = weight.size(dim=3)\n",
    "  wt_y = weight.size(dim=2)\n",
    "  c = weight.size(dim=1)\n",
    "  k = weight.size(dim=0)\n",
    "  out = torch.empty(size=(1,k, int((in_x-wt_x)/stride[0]+1), int((in_y-wt_y)/stride[1]+1)))\n",
    "\n",
    "  mult_count = 0\n",
    "  for i in range(k):\n",
    "    comp_wt = comp_vector(weight[i])\n",
    "    out[0][i], num =conv_compressed(comp_in, comp_wt, stride[0])\n",
    "    out[0][i] += bias[i]\n",
    "    mult_count+=num\n",
    "#   print(out)\n",
    "  return (out,mult_count)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# ALGO 3\n",
    "def compute_weight_list(kernel):    \n",
    "    kernels = []\n",
    "    filter_count = kernel.shape[0]\n",
    "    depth = kernel.shape[1]\n",
    "    height = kernel.shape[2]\n",
    "    width = kernel.shape[3]\n",
    "    for f in range(filter_count):\n",
    "        weight_list = []\n",
    "        for k in range(depth):\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    w = kernel[f][k][i][j]\n",
    "                    if w < 0:\n",
    "                        weight_list.append(tuple((w, k, i, j)))\n",
    "        sorted_weight_list = sorted(weight_list, key = lambda x: x[0])\n",
    "        kernels.append(sorted_weight_list)\n",
    "    return kernels\n",
    "\n",
    "def compute_conv_onlypred(img, weight_list, weights, r, c, bias=0):\n",
    "    img_out_cell = 0\n",
    "    conv_mult_count = 0\n",
    "    depth = weights.shape[0]\n",
    "    height = weights.shape[1]\n",
    "    width = weights.shape[2]\n",
    "    for k in range(depth):\n",
    "        for i in range(width):\n",
    "            for j in range(height):          \n",
    "                if weights[k][i][j]>0:\n",
    "                  #  and r+i<img.shape[2] and c+j<img.shape[3]\n",
    "                    conv_mult_count += 1 \n",
    "                    img_out_cell += img[0][k][r+i][c+j]*weights[k][i][j]\n",
    "\n",
    "    img_out_cell+=bias\n",
    "    \n",
    "    for tup in weight_list:\n",
    "        conv_mult_count += 1\n",
    "        # if r+tup[2]>=img.shape[2] or c+tup[3]>=img.shape[3]:\n",
    "        #   continue\n",
    "        img_out_cell += tup[0]*img[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "        if img_out_cell < 0:\n",
    "            break\n",
    "    return img_out_cell, conv_mult_count\n",
    "\n",
    "def compute_filter_conv_onlypred(img, weight_list, weights, kernel_id,stride=(1,1), padding=(0,0), bias=0):\n",
    "    width_out = int((img.shape[3]-weights.shape[2])/stride[1]+1)\n",
    "    height_out = int((img.shape[2]-weights.shape[1])/stride[0]+1)\n",
    "    img_out_channel = torch.zeros(width_out,height_out)\n",
    "    filter_mult_count = 0\n",
    "    # print(img.shape[2]+2*padding[0]-weights.shape[1], img.shape[3]+2*padding[1]-weights.shape[2])\n",
    "    for r in range(0,img.shape[2]-weights.shape[1]+1,stride[0]):\n",
    "        for c in range(0,img.shape[3]-weights.shape[2]+1,stride[1]):\n",
    "            r_out = int(r/stride[0])\n",
    "            c_out = int(c/stride[1])\n",
    "            # print(r_out, c_out)\n",
    "            img_out_channel[r_out][c_out], mult_count = compute_conv_onlypred(img, weight_list, weights, r, c, bias)\n",
    "            # img_out_channel[r_out][c_out] += bias\n",
    "            filter_mult_count += mult_count\n",
    "    return img_out_channel, filter_mult_count\n",
    "\n",
    "def myconv2d_onlypred(img, weights, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    img = torch.nn.functional.pad(img, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "    layer_mult_count = 0\n",
    "    filter_count = weights.shape[0]\n",
    "    depth = weights.shape[1]\n",
    "    height = weights.shape[2]\n",
    "    width = weights.shape[3]\n",
    "    channels_out=filter_count\n",
    "    width_out = int((img.shape[3]-width)/stride[1]+1)\n",
    "    height_out = int((img.shape[2]-height)/stride[0]+1)\n",
    "    img_conv_output = torch.zeros(1, channels_out, width_out, height_out)\n",
    "    filters_list = compute_weight_list(weights)\n",
    "    for kernel_id in range(filter_count):\n",
    "        if kernel_id%8==0:\n",
    "            print(\"kernel_id\", kernel_id)\n",
    "        weight_list = filters_list[kernel_id]\n",
    "        img_conv_channel, mult_count = compute_filter_conv_onlypred(img, weight_list, weights[kernel_id], kernel_id, stride, padding, bias[kernel_id])\n",
    "        img_conv_output[0][kernel_id] = img_conv_channel\n",
    "        layer_mult_count += mult_count\n",
    "    return (img_conv_output, layer_mult_count)\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "# ALGO 4\n",
    "class comp_vector_pred():\n",
    "  def __init__(self, arr):\n",
    "    self.y = arr.size(dim=2)\n",
    "    self.x = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.pos_vector = [] #stores tuples of (data, index)\n",
    "    self.neg_vector = []\n",
    "\n",
    "    for k in range(self.c):\n",
    "            for i in range(self.x):\n",
    "                for j in range(self.y):\n",
    "                    w = arr[k][i][j]\n",
    "                    if w > 0:\n",
    "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
    "                    elif w<0:\n",
    "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
    "\n",
    "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
    "\n",
    "  def get_pos_vector(self):\n",
    "    return self.pos_vector\n",
    "\n",
    "  def get_neg_vector(self):\n",
    "    return self.neg_vector\n",
    "\n",
    "\n",
    "def compute_conv_sparsepred(input, weight, comp_wt, r, c, bias=0):\n",
    "  img_out_cell = 0\n",
    "  conv_mult_count = 0\n",
    "  pos = comp_wt.get_pos_vector()\n",
    "  neg = comp_wt.get_neg_vector()\n",
    "\n",
    "  x = weight.shape[1]\n",
    "  y = weight.shape[2]\n",
    "  k = weight.shape[0]\n",
    "\n",
    "  mult_nonzero = 0\n",
    "  for channel in range(k):\n",
    "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
    "    inp_nonzero = np.flatnonzero(inp_window)\n",
    "    wt_nonzero = np.flatnonzero(weight[channel])\n",
    "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
    "    mult_nonzero += common\n",
    "\n",
    "  for tup in pos:\n",
    "    # if(r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]):\n",
    "    #   continue\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  img_out_cell+=bias\n",
    "\n",
    "  for tup in neg:\n",
    "    if img_out_cell < 0:\n",
    "      break\n",
    "    # if(r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]):\n",
    "    #   continue\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  return img_out_cell, conv_mult_count, mult_nonzero\n",
    "\n",
    "\n",
    "def compute_filter_conv_sparsepred(input, weights, comp_wt, width_out, height_out,stride=(1,1), padding=(0,0), bias=0):\n",
    "#     print('called compute_filter_conv')\n",
    "    img_out_channel = torch.zeros(width_out, height_out)\n",
    "    filter_mult_count = 0\n",
    "    filter_calc_mult = 0\n",
    "    for r in range(0,input.shape[2]-weights.shape[1]+1,stride[0]):\n",
    "        for c in range(0,input.shape[3]-weights.shape[2]+1,stride[1]):\n",
    "            r_out = int(r/stride[0])\n",
    "            c_out = int(c/stride[1])\n",
    "            img_out_channel[r_out][c_out], mult_count, calc_mult = compute_conv_sparsepred(input, weights,comp_wt, r, c, bias)\n",
    "            filter_mult_count += mult_count\n",
    "            filter_calc_mult += calc_mult\n",
    "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
    "\n",
    "\n",
    "def myconv2d_sparse_pred(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "    in_x = input.shape[2]\n",
    "    in_y = input.shape[3]\n",
    "    wt_x = weight.shape[2]\n",
    "    wt_y = weight.shape[3]\n",
    "    c = weight.shape[1]\n",
    "    filter_count = weight.shape[0]\n",
    "    w = int((in_x-wt_x)//stride[0]+1)\n",
    "    h = int((in_y-wt_y)//stride[1]+1)\n",
    "    out = torch.empty(size=(1, filter_count, w, h))\n",
    "\n",
    "    mult_count = 0\n",
    "    calc_mult = 0\n",
    "    for i in range(filter_count):\n",
    "        comp_wt = comp_vector_pred(weight[i])\n",
    "        out[0][i], num1, num2 =compute_filter_conv_sparsepred(input, weight[i], comp_wt, w, h,stride, padding, bias[i])\n",
    "        # out[0][i] += bias[i]\n",
    "        mult_count+=num1\n",
    "        calc_mult+=num2\n",
    "    # mult_count is predictive sparse(weight only)\n",
    "    # calc_mult is baseline 2(sparse non-predictive)\n",
    "#     print(out)\n",
    "    return (out,mult_count)\n",
    "###################################################################################\n",
    "\n",
    "# ALGO 5\n",
    "class comp_vector_pred_twosided():\n",
    "  def __init__(self, arr):\n",
    "    self.x = arr.size(dim=2)\n",
    "    self.y = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.pos_vector = [] #stores tuples of (data, index)\n",
    "    self.neg_vector = []\n",
    "\n",
    "    for k in range(self.c):\n",
    "            for i in range(self.y):\n",
    "                for j in range(self.x):\n",
    "                    w = arr[k][i][j]\n",
    "                    if w > 0:\n",
    "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
    "                    elif w<0:\n",
    "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
    "\n",
    "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
    "\n",
    "  def get_pos_vector(self):\n",
    "    return self.pos_vector\n",
    "\n",
    "  def get_neg_vector(self):\n",
    "    return self.neg_vector\n",
    "\n",
    "\n",
    "def compute_conv_sparsepred_twosided(input, weight, comp_wt, r, c,bias=0):\n",
    "  img_out_cell = 0\n",
    "  conv_mult_count = 0\n",
    "  pos = comp_wt.get_pos_vector()\n",
    "  neg = comp_wt.get_neg_vector()\n",
    "\n",
    "  x = weight.shape[1]\n",
    "  y = weight.shape[2]\n",
    "  k = weight.shape[0]\n",
    "\n",
    "  mult_nonzero = 0\n",
    "  for channel in range(k):\n",
    "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
    "    inp_nonzero = np.flatnonzero(inp_window)\n",
    "    wt_nonzero = np.flatnonzero(weight[channel])\n",
    "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
    "    mult_nonzero += common\n",
    "\n",
    "  for tup in pos:\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "      #  or r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]\n",
    "      continue\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  img_out_cell+=bias\n",
    "\n",
    "  # idx = 0\n",
    "  # while img_out_cell>=0 and idx<len(neg):\n",
    "  #   tup = neg[idx]\n",
    "  #   if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "  #     continue\n",
    "  #   conv_mult_count += 1\n",
    "  #   img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "  #   idx+=1\n",
    "\n",
    "  for tup in neg:\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "      #  or r+tup[2]>=input.shape[2] or c+tup[3]>=input.shape[3]\n",
    "      continue\n",
    "    if img_out_cell < 0:\n",
    "      break\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  return img_out_cell, conv_mult_count, mult_nonzero\n",
    "\n",
    "\n",
    "def compute_filter_conv_sparsepred_twosided(input, weights, comp_wt, width_out, height_out,stride=(1,1), padding=(0,0), bias=0):\n",
    "    img_out_channel = torch.zeros(width_out, height_out)\n",
    "    filter_mult_count = 0\n",
    "    filter_calc_mult = 0\n",
    "    # print(input.shape)\n",
    "    # print(weights.shape)\n",
    "    # print(len(stride))\n",
    "    # print(len(padding))\n",
    "    for r in range(0,input.shape[2]-weights.shape[1]+1,stride[0]):\n",
    "        for c in range(0,input.shape[3]-weights.shape[2]+1,stride[1]):\n",
    "            r_out = int(r/stride[0])\n",
    "            c_out = int(c/stride[1])\n",
    "            img_out_channel[r_out][c_out], mult_count, calc_mult = compute_conv_sparsepred_twosided(input, weights,comp_wt, r, c, bias)\n",
    "            # img_out_channel[r][c] += bias \n",
    "            # Bias added in compute_conv_sparsepred_twosided\n",
    "            filter_mult_count += mult_count\n",
    "            filter_calc_mult += calc_mult\n",
    "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
    "\n",
    "\n",
    "def myconv2d_sparse_pred_twosided(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "  in_x = input.shape[2]\n",
    "  in_y = input.shape[3]\n",
    "  wt_x = weight.shape[2]\n",
    "  wt_y = weight.shape[3]\n",
    "  c = weight.shape[1]\n",
    "  filter_count = weight.shape[0]\n",
    "  w = int((in_x-wt_x)//stride[0]+1)\n",
    "  h = int((in_y-wt_y)//stride[1]+1)\n",
    "  # print(w,h)\n",
    "  out = torch.empty(size=(1, filter_count, w, h))\n",
    "\n",
    "  mult_count = 0\n",
    "  calc_mult = 0\n",
    "  for i in range(filter_count):\n",
    "    comp_wt = comp_vector_pred_twosided(weight[i])\n",
    "    out[0][i], num1, num2 =compute_filter_conv_sparsepred_twosided(input, weight[i], comp_wt, w, h,stride=stride,padding=padding,bias=bias[i])\n",
    "    mult_count+=num1\n",
    "    calc_mult+=num2\n",
    "  \n",
    "  return (out,mult_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkUJl1GPSz7u"
   },
   "source": [
    "**Defining custom Conv2D Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "6duNFz6pSsuR"
   },
   "outputs": [],
   "source": [
    "class Custom_Conv2d(torch.nn.modules.conv._ConvNd):\n",
    "    \"\"\"\n",
    "    Implements a standard convolution layer that can be used as a regular module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        stride = (stride, stride)\n",
    "        padding = (padding, padding)\n",
    "        dilation = (dilation, dilation)\n",
    "        super(Custom_Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, (0, 0), groups, bias, padding_mode)\n",
    "\n",
    "    def conv2d_forward(self, input, weight):\n",
    "        return myconv2d_onlypred(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv2d_forward(input, self.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSoVmFiKTGMx"
   },
   "source": [
    "**Defining custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "5vU3IzS1srIo"
   },
   "outputs": [],
   "source": [
    "# empty arrays for storing activation sparsities\n",
    "c1 = []\n",
    "c2 = []\n",
    "c3 = []\n",
    "c4 = []\n",
    "c5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "kIg17oiX0-nc"
   },
   "outputs": [],
   "source": [
    "# empty arrays for storing #MACops per layer\n",
    "m1 = []\n",
    "m2 = []\n",
    "m3 = []\n",
    "m4 = []\n",
    "m5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "5DIHcqOOTBKe"
   },
   "outputs": [],
   "source": [
    "class CustomAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            Custom_Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "            Custom_Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "            Custom_Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            Custom_Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            Custom_Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print('Sparsity of CONV1 activations: ', (1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "        c1.append((1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "        out, macops1 = self.features[0](x)             \n",
    "        m1.append(macops1)\n",
    "        out = out.to(device)\n",
    "#         print('features layer 0 done')\n",
    "        out = self.features[1](out)\n",
    "        out = self.features[2](out)\n",
    "        \n",
    "        # print('Sparsity of CONV2 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c2.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops2 = self.features[3](out)\n",
    "        m2.append(macops2)\n",
    "        out = out.to(device)\n",
    "#         print('features layer 3 done')\n",
    "        out = self.features[4](out)\n",
    "        out = self.features[5](out)\n",
    "        \n",
    "        # print('Sparsity of CONV3 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c3.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops3 = self.features[6](out)\n",
    "        m3.append(macops3)\n",
    "        out = out.to(device)\n",
    "#         print('features layer 6 done')\n",
    "        out = self.features[7](out)\n",
    "        \n",
    "        # print('Sparsity of CONV4 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c4.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops4 = self.features[8](out)\n",
    "        m4.append(macops4)\n",
    "        out = out.to(device)\n",
    "#         print('features layer 8 done')\n",
    "        out = self.features[9](out)\n",
    "        \n",
    "        c5.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        # print('Sparsity of CONV5 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        out, macops5 = self.features[10](out)\n",
    "        m5.append(macops5)\n",
    "        out = out.to(device)\n",
    "#         print('features layer 10 done')\n",
    "        out = self.features[11](out)\n",
    "        out = self.features[12](out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3s82aEvUACf"
   },
   "source": [
    "\n",
    "\n",
    "**Instantiating a custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "xaR1XvzgT7cK"
   },
   "outputs": [],
   "source": [
    "cust_alexnet = CustomAlexNet(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jpwa7XQFUiCU"
   },
   "source": [
    "**Loading pretrained AlexNet weights into our custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "FDY_ZerKUJwk",
    "outputId": "8403f8c4-8fc8-4c59-d474-17ab6337055c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomAlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Custom_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Custom_Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Custom_Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Custom_Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Custom_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alexnet = torch.load(r'alexnet.pth')\n",
    "cust_alexnet.load_state_dict(torch.load(r'alexnet_unnormalised.pth'))\n",
    "cust_alexnet.to(device)\n",
    "cust_alexnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNXnVogrcvjr"
   },
   "source": [
    "# Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfZGMh9Ccx4X"
   },
   "source": [
    "**Accuracy & Layer-wise Activation Sparsities(Dynamic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "VpctHowbcIWn"
   },
   "outputs": [],
   "source": [
    "def test_model(cust_alexnet):\n",
    "  cust_alexnet.to(device)\n",
    "  cust_alexnet.eval()\n",
    "#   print('sent model to device')\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  m1.clear()\n",
    "  m2.clear()\n",
    "  m3.clear()\n",
    "  m4.clear()\n",
    "  m5.clear()\n",
    "  c1.clear()\n",
    "  c2.clear()\n",
    "  c3.clear()\n",
    "  c4.clear()\n",
    "  c5.clear()\n",
    "  with torch.no_grad():\n",
    "      for data in test_loader:\n",
    "          if(total>=1):\n",
    "            break\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "#           print('sent image and labels to device')\n",
    "          outputs = cust_alexnet(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))\n",
    "  print(\"CONV1 #MACops(avg):\", sum(m1)/len(m1))\n",
    "  print(\"CONV2 #MACops(avg):\", sum(m2)/len(m2))\n",
    "  print(\"CONV3 #MACops(avg):\", sum(m3)/len(m3))\n",
    "  print(\"CONV4 #MACops(avg):\", sum(m4)/len(m4))\n",
    "  print(\"CONV5 #MACops(avg):\", sum(m5)/len(m5))\n",
    "\n",
    "  print(\"CONV1 activation sparsity(avg):\", sum(c1)/len(c1))\n",
    "  print(\"CONV2 activation sparsity(avg):\", sum(c2)/len(c2))\n",
    "  print(\"CONV3 activation sparsity(avg):\", sum(c3)/len(c3))\n",
    "  print(\"CONV4 activation sparsity(avg):\", sum(c4)/len(c4))\n",
    "  print(\"CONV5 activation sparsity(avg):\", sum(c5)/len(c5))\n",
    "  # sum(c2)/len(c2)\n",
    "  print(len(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx7IdeTXc5ag"
   },
   "source": [
    "**Weight Sparsity (static)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "stSC9uncVI9r"
   },
   "outputs": [],
   "source": [
    "def get_alex_w_sparsities(cust_alexnet):\n",
    "  conv_indices = [0, 3, 6, 8, 10]\n",
    "\n",
    "  for i in range(5):\n",
    "    layer_index = conv_indices[i]\n",
    "\n",
    "    print(\n",
    "        \"Sparsity in conv{:}.weight: {:.2f}%\".format(i+1, \n",
    "            100. * float(torch.sum(cust_alexnet.features[layer_index].weight == 0))\n",
    "            / float(cust_alexnet.features[layer_index].weight.nelement())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ushoqca7d5s5",
    "outputId": "4c9c1a2d-3cc1-4923-9085-803c046abb6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 0.00%\n",
      "Sparsity in conv2.weight: 0.00%\n",
      "Sparsity in conv3.weight: 0.00%\n",
      "Sparsity in conv4.weight: 0.00%\n",
      "Sparsity in conv5.weight: 0.00%\n"
     ]
    }
   ],
   "source": [
    "get_alex_w_sparsities(cust_alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5w4M9T6yS_K"
   },
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomAlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Custom_Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Custom_Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Custom_Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Custom_Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Custom_Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_90 = CustomAlexNet(10)\n",
    "alex_90.load_state_dict(torch.load(r'alexnet.pth'))\n",
    "\n",
    "for name, module in alex_90.named_modules():\n",
    "    # prune 90% of connections in all 2D-conv layers\n",
    "    if isinstance(module, Custom_Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 25.00%\n",
      "Sparsity in conv2.weight: 25.00%\n",
      "Sparsity in conv3.weight: 25.00%\n",
      "Sparsity in conv4.weight: 25.00%\n",
      "Sparsity in conv5.weight: 25.00%\n"
     ]
    }
   ],
   "source": [
    "get_alex_w_sparsities(alex_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 83.35 %\n",
      "CONV1 #MACops(avg): 1.0\n",
      "CONV2 #MACops(avg): 1.0\n",
      "CONV3 #MACops(avg): 1.0\n",
      "CONV4 #MACops(avg): 1.0\n",
      "CONV5 #MACops(avg): 1.0\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.4914952580094337\n",
      "CONV3 activation sparsity(avg): 0.7596199704170227\n",
      "CONV4 activation sparsity(avg): 0.8848602611422539\n",
      "CONV5 activation sparsity(avg): 0.8694982788264751\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "test_model(alex_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomAlexNet(10)\n",
    "model.load_state_dict(torch.load(r'alexnet_unnormalised.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[3], 'weight'),\n",
    "    (model.features[6], 'weight'),\n",
    "    (model.features[8], 'weight'),\n",
    "    (model.features[10], 'weight'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 0.00%\n",
      "Sparsity in conv2.weight: 0.00%\n",
      "Sparsity in conv3.weight: 0.00%\n",
      "Sparsity in conv4.weight: 0.00%\n",
      "Sparsity in conv5.weight: 0.00%\n"
     ]
    }
   ],
   "source": [
    "get_alex_w_sparsities(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 16\n",
      "kernel_id 24\n",
      "kernel_id 32\n",
      "kernel_id 40\n",
      "kernel_id 48\n",
      "kernel_id 56\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 16\n",
      "kernel_id 24\n",
      "kernel_id 32\n",
      "kernel_id 40\n",
      "kernel_id 48\n",
      "kernel_id 56\n",
      "kernel_id 64\n",
      "kernel_id 72\n",
      "kernel_id 80\n",
      "kernel_id 88\n",
      "kernel_id 96\n",
      "kernel_id 104\n",
      "kernel_id 112\n",
      "kernel_id 120\n",
      "kernel_id 128\n",
      "kernel_id 136\n",
      "kernel_id 144\n",
      "kernel_id 152\n",
      "kernel_id 160\n",
      "kernel_id 168\n",
      "kernel_id 176\n",
      "kernel_id 184\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 16\n",
      "kernel_id 24\n",
      "kernel_id 32\n",
      "kernel_id 40\n",
      "kernel_id 48\n",
      "kernel_id 56\n",
      "kernel_id 64\n",
      "kernel_id 72\n",
      "kernel_id 80\n",
      "kernel_id 88\n",
      "kernel_id 96\n",
      "kernel_id 104\n",
      "kernel_id 112\n",
      "kernel_id 120\n",
      "kernel_id 128\n",
      "kernel_id 136\n",
      "kernel_id 144\n",
      "kernel_id 152\n",
      "kernel_id 160\n",
      "kernel_id 168\n",
      "kernel_id 176\n",
      "kernel_id 184\n",
      "kernel_id 192\n",
      "kernel_id 200\n",
      "kernel_id 208\n",
      "kernel_id 216\n",
      "kernel_id 224\n",
      "kernel_id 232\n",
      "kernel_id 240\n",
      "kernel_id 248\n",
      "kernel_id 256\n",
      "kernel_id 264\n",
      "kernel_id 272\n",
      "kernel_id 280\n",
      "kernel_id 288\n",
      "kernel_id 296\n",
      "kernel_id 304\n",
      "kernel_id 312\n",
      "kernel_id 320\n",
      "kernel_id 328\n",
      "kernel_id 336\n",
      "kernel_id 344\n",
      "kernel_id 352\n",
      "kernel_id 360\n",
      "kernel_id 368\n",
      "kernel_id 376\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 16\n",
      "kernel_id 24\n",
      "kernel_id 32\n",
      "kernel_id 40\n",
      "kernel_id 48\n",
      "kernel_id 56\n",
      "kernel_id 64\n",
      "kernel_id 72\n",
      "kernel_id 80\n",
      "kernel_id 88\n",
      "kernel_id 96\n",
      "kernel_id 104\n",
      "kernel_id 112\n",
      "kernel_id 120\n",
      "kernel_id 128\n",
      "kernel_id 136\n",
      "kernel_id 144\n",
      "kernel_id 152\n",
      "kernel_id 160\n",
      "kernel_id 168\n",
      "kernel_id 176\n",
      "kernel_id 184\n",
      "kernel_id 192\n",
      "kernel_id 200\n",
      "kernel_id 208\n",
      "kernel_id 216\n",
      "kernel_id 224\n",
      "kernel_id 232\n",
      "kernel_id 240\n",
      "kernel_id 248\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 16\n",
      "kernel_id 24\n",
      "kernel_id 32\n",
      "kernel_id 40\n",
      "kernel_id 48\n",
      "kernel_id 56\n",
      "kernel_id 64\n",
      "kernel_id 72\n",
      "kernel_id 80\n",
      "kernel_id 88\n",
      "kernel_id 96\n",
      "kernel_id 104\n",
      "kernel_id 112\n",
      "kernel_id 120\n",
      "kernel_id 128\n",
      "kernel_id 136\n",
      "kernel_id 144\n",
      "kernel_id 152\n",
      "kernel_id 160\n",
      "kernel_id 168\n",
      "kernel_id 176\n",
      "kernel_id 184\n",
      "kernel_id 192\n",
      "kernel_id 200\n",
      "kernel_id 208\n",
      "kernel_id 216\n",
      "kernel_id 224\n",
      "kernel_id 232\n",
      "kernel_id 240\n",
      "kernel_id 248\n",
      "Accuracy of the network on the test images: 100.00 %\n",
      "CONV1 #MACops(avg): 60270889.0\n",
      "CONV2 #MACops(avg): 141421421.0\n",
      "CONV3 #MACops(avg): 75561878.0\n",
      "CONV4 #MACops(avg): 105061055.0\n",
      "CONV5 #MACops(avg): 56442685.0\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.7962534427642822\n",
      "CONV3 activation sparsity(avg): 0.7471030950546265\n",
      "CONV4 activation sparsity(avg): 0.861470639705658\n",
      "CONV5 activation sparsity(avg): 0.8165218234062195\n",
      "1\n",
      "13876.851637363434\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(model)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
