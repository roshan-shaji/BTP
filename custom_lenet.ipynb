{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdb11f7",
   "metadata": {},
   "source": [
    "This notebook has custom AlexNet. It measures:\n",
    "\n",
    "\n",
    "1.   Sparsity of weights(one-time)\n",
    "2.   Layerwise CONV layer activation sparsities\n",
    "3.   Accuracy of the model\n",
    "4. Layerwise #MAC ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303bdf7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1b5596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ae0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "#                                                   transforms.Normalize(mean = (0.1325,), std = (0.3105,))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38f287",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fee6fa",
   "metadata": {},
   "source": [
    "**Custom conv2d function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4c5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myconv2d(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    \"\"\"\n",
    "    Function to process an input with a standard convolution\n",
    "    \"\"\"\n",
    "    mul_count = 0\n",
    "#     print('input', input.shape)\n",
    "#     print('wt', weight.shape)\n",
    "    batch_size, in_channels, in_h, in_w = input.shape\n",
    "    out_channels, in_channels, kh, kw = weight.shape\n",
    "    out_h = int((in_h - kh + 2 * padding[0]) / stride[0] + 1)\n",
    "    out_w = int((in_w - kw + 2 * padding[1]) / stride[1] + 1)\n",
    "    unfold = torch.nn.Unfold(kernel_size=(kh, kw), dilation=dilation, padding=padding, stride=stride)\n",
    "    inp_unf = unfold(input)\n",
    "    w_ = weight.view(weight.size(0), -1).t()\n",
    "    if bias is None:\n",
    "        out_unf = inp_unf.transpose(1, 2).matmul(w_).transpose(1, 2)\n",
    "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
    "    else:\n",
    "        out_unf = (inp_unf.transpose(1, 2).matmul(w_) + bias).transpose(1, 2)\n",
    "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
    "    out = out_unf.view(batch_size, out_channels, out_h, out_w)\n",
    "#     print(out)\n",
    "    return (out.float(), mul_count)\n",
    "    # return out.float()\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "class comp_vector():\n",
    "  def __init__(self, arr):\n",
    "    self.x = arr.size(dim=2)\n",
    "    self.y = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.index_vector = []\n",
    "    self.data_vector = []\n",
    "    for i in range(self.c):\n",
    "      # print(arr[i])\n",
    "      self.index_vector.append(np.flatnonzero(arr[i].cpu()))\n",
    "      self.data_vector.append(arr[i].ravel()[self.index_vector[-1]])\n",
    "\n",
    "    # index_vector = np.flatnonzero(arr)\n",
    "    # data_vector = arr.ravel()[index_vector]\n",
    "\n",
    "  def get_index_vector(self):\n",
    "    return self.index_vector\n",
    "\n",
    "  def get_data_vector(self):\n",
    "    return self.data_vector\n",
    "\n",
    "\n",
    "def conv_compressed(comp_inp, comp_wt, stride=1):\n",
    "#     print('called conv_compressed')\n",
    "    acc_x, acc_y, acc_c = int((comp_inp.x - comp_wt.x)//stride  + 1) , int((comp_inp.y - comp_wt.y)//stride  +1), comp_wt.c\n",
    "#     print(acc_x, acc_y, acc_c)\n",
    "    mult_count = 0\n",
    "    # print(acc_x, acc_y, acc_c)\n",
    "    acc_buf = torch.FloatTensor(acc_x, acc_y).zero_()\n",
    "    inp_index_vector = comp_inp.get_index_vector()\n",
    "    inp_data_vector = comp_inp.get_data_vector()\n",
    "    wt_index_vector = comp_wt.get_index_vector()\n",
    "    wt_data_vector = comp_wt.get_data_vector()\n",
    "    # print(inp_index_vector[0])\n",
    "    # print(len(inp_index_vector[0]))\n",
    "    for c in range(acc_c):\n",
    "      for i in range(len(inp_index_vector[c])):\n",
    "        for j in range(len(wt_index_vector[c])):\n",
    "          inp_x = inp_index_vector[c][i]//comp_inp.x\n",
    "          inp_y = inp_index_vector[c][i]%comp_inp.y\n",
    "          wt_x = wt_index_vector[c][j]//comp_wt.x\n",
    "          wt_y = wt_index_vector[c][j]%comp_wt.y\n",
    "\n",
    "          out_x = (inp_x - wt_x)\n",
    "          out_y = (inp_y- wt_y)\n",
    "          if out_x%stride==0 and out_y%stride==0:\n",
    "            out_x = out_x//stride\n",
    "            out_y = out_y//stride\n",
    "            # print(out_x, out_y,c,i,j,)\n",
    "            if 0<=out_x<acc_x and 0<=out_y<acc_y:\n",
    "              # print(\"yes\")\n",
    "              acc_buf[out_x][out_y]+=float(inp_data_vector[c][i] * wt_data_vector[c][j])\n",
    "              mult_count +=1\n",
    "    \n",
    "    return (acc_buf,mult_count)\n",
    "\n",
    "def myconv2d_sparse(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "#   print(input.size())\n",
    "  comp_in = comp_vector(input[0])\n",
    "  in_x = input.size(dim=3)\n",
    "  in_y = input.size(dim=2)\n",
    "  wt_x = weight.size(dim=3)\n",
    "  wt_y = weight.size(dim=2)\n",
    "  c = weight.size(dim=1)\n",
    "  k = weight.size(dim=0)\n",
    "  out = torch.empty(size=(1,k, int((in_x-wt_x)/stride[0]+1), int((in_y-wt_y)/stride[1]+1)))\n",
    "\n",
    "  mult_count = 0\n",
    "  for i in range(k):\n",
    "    comp_wt = comp_vector(weight[i])\n",
    "    out[0][i], num =conv_compressed(comp_in, comp_wt, stride[0])\n",
    "    out[0][i] += bias[i]\n",
    "    mult_count+=num\n",
    "#   print(out)\n",
    "  return (out,mult_count)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# ALGO 3\n",
    "def compute_weight_list(kernel):    \n",
    "    kernels = []\n",
    "    filter_count = kernel.shape[0]\n",
    "    depth = kernel.shape[1]\n",
    "    height = kernel.shape[2]\n",
    "    width = kernel.shape[3]\n",
    "    for f in range(filter_count):\n",
    "        weight_list = []\n",
    "        for k in range(depth):\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    w = kernel[f][k][i][j]\n",
    "                    if w < 0:\n",
    "                        weight_list.append(tuple((w, k, i, j)))\n",
    "        sorted_weight_list = sorted(weight_list, key = lambda x: x[0])\n",
    "        kernels.append(sorted_weight_list)\n",
    "    return kernels\n",
    "\n",
    "def compute_conv_onlypred(img, weight_list, weights, r, c, bias=0):\n",
    "    img_out_cell = 0\n",
    "    conv_mult_count = 0\n",
    "    depth = weights.shape[0]\n",
    "    height = weights.shape[1]\n",
    "    width = weights.shape[2]\n",
    "    for k in range(depth):\n",
    "        for i in range(width):\n",
    "            for j in range(height):          \n",
    "                if weights[k][i][j] > 0:\n",
    "                    conv_mult_count += 1 \n",
    "                    img_out_cell += img[0][k][r+i][c+j]*weights[k][i][j]\n",
    "\n",
    "    img_out_cell+=bias\n",
    "    \n",
    "    for tup in weight_list:\n",
    "        conv_mult_count += 1\n",
    "        img_out_cell += tup[0]*img[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "        if img_out_cell < 0:\n",
    "            break\n",
    "    return img_out_cell, conv_mult_count\n",
    "\n",
    "def compute_filter_conv_onlypred(img, weight_list, weights, kernel_id,stride=(1,1), padding=(0,0), bias=0):\n",
    "    width_out = int((img.shape[3]+2*padding[1]-weights.shape[2])/stride[1]+1)\n",
    "    height_out = int((img.shape[2]+2*padding[0]-weights.shape[1])/stride[0]+1)\n",
    "    img_out_channel = torch.zeros(width_out,height_out)\n",
    "    filter_mult_count = 0\n",
    "    # print(img.shape[2]+2*padding[0]-weights.shape[1], img.shape[3]+2*padding[1]-weights.shape[2])\n",
    "    for r in range(0,img.shape[2]+2*padding[0]-weights.shape[1]+1,stride[0]):\n",
    "        for c in range(0,img.shape[3]+2*padding[1]-weights.shape[2]+1,stride[1]):\n",
    "            r_out = int(r/stride[0])\n",
    "            c_out = int(c/stride[1])\n",
    "            # print(r_out, c_out)\n",
    "            img_out_channel[r_out][c_out], mult_count = compute_conv_onlypred(img, weight_list, weights, r, c, bias)\n",
    "            # img_out_channel[r_out][c_out] += bias\n",
    "            filter_mult_count += mult_count\n",
    "    return img_out_channel, filter_mult_count\n",
    "\n",
    "def myconv2d_onlypred(img, weights, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    layer_mult_count = 0\n",
    "    filter_count = weights.shape[0]\n",
    "    depth = weights.shape[1]\n",
    "    height = weights.shape[2]\n",
    "    width = weights.shape[3]\n",
    "    channels_out=filter_count\n",
    "    width_out = int((img.shape[3]+2*padding[1]-width)/stride[1]+1)\n",
    "    height_out = int((img.shape[2]+2*padding[0]-height)/stride[0]+1)\n",
    "    img_conv_output = torch.zeros(1, channels_out, width_out, height_out)\n",
    "    filters_list = compute_weight_list(weights)\n",
    "    for kernel_id in range(filter_count):\n",
    "        if kernel_id%8==0:\n",
    "            print(\"kernel_id\", kernel_id)\n",
    "        weight_list = filters_list[kernel_id]\n",
    "        img_conv_channel, mult_count = compute_filter_conv_onlypred(img, weight_list, weights[kernel_id], kernel_id, stride, padding, bias[kernel_id])\n",
    "        img_conv_output[0][kernel_id] = img_conv_channel\n",
    "        layer_mult_count += mult_count\n",
    "    return (img_conv_output, layer_mult_count)\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "# ALGO 4\n",
    "class comp_vector_pred():\n",
    "  def __init__(self, arr):\n",
    "    self.y = arr.size(dim=2)\n",
    "    self.x = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.pos_vector = [] #stores tuples of (data, index)\n",
    "    self.neg_vector = []\n",
    "\n",
    "    for k in range(self.c):\n",
    "            for i in range(self.x):\n",
    "                for j in range(self.y):\n",
    "                    w = arr[k][i][j]\n",
    "                    if w > 0:\n",
    "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
    "                    elif w<0:\n",
    "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
    "\n",
    "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
    "\n",
    "  def get_pos_vector(self):\n",
    "    return self.pos_vector\n",
    "\n",
    "  def get_neg_vector(self):\n",
    "    return self.neg_vector\n",
    "\n",
    "\n",
    "def compute_conv_sparsepred(input, weight, comp_wt, r, c, bias=0):\n",
    "  img_out_cell = 0\n",
    "  conv_mult_count = 0\n",
    "  pos = comp_wt.get_pos_vector()\n",
    "  neg = comp_wt.get_neg_vector()\n",
    "\n",
    "  x = weight.shape[1]\n",
    "  y = weight.shape[2]\n",
    "  k = weight.shape[0]\n",
    "\n",
    "  mult_nonzero = 0\n",
    "  for channel in range(k):\n",
    "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
    "    inp_nonzero = np.flatnonzero(inp_window)\n",
    "    wt_nonzero = np.flatnonzero(weight[channel])\n",
    "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
    "    mult_nonzero += common\n",
    "\n",
    "  for tup in pos:\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  img_out_cell+=bias\n",
    "\n",
    "  for tup in neg:\n",
    "    if img_out_cell < 0:\n",
    "      break\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  return img_out_cell, conv_mult_count, mult_nonzero\n",
    "\n",
    "\n",
    "def compute_filter_conv_sparsepred(input, weights, comp_wt, width_out, height_out,stride=(1,1), padding=(0,0), bias=0):\n",
    "#     print('called compute_filter_conv')\n",
    "    img_out_channel = torch.zeros(width_out, height_out)\n",
    "    filter_mult_count = 0\n",
    "    filter_calc_mult = 0\n",
    "    for r in range(0,input.shape[2]+2*padding[0]-weights.shape[1]+1,stride[0]):\n",
    "        for c in range(0,input.shape[3]+2*padding[1]-weights.shape[2]+1,stride[1]):\n",
    "            r_out = int(r/stride[0])\n",
    "            c_out = int(c/stride[1])\n",
    "            img_out_channel[r_out][c_out], mult_count, calc_mult = compute_conv_sparsepred(input, weights,comp_wt, r, c, bias)\n",
    "            filter_mult_count += mult_count\n",
    "            filter_calc_mult += calc_mult\n",
    "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
    "\n",
    "\n",
    "def myconv2d_sparse_pred(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "    in_x = input.shape[2]\n",
    "    in_y = input.shape[3]\n",
    "    wt_x = weight.shape[2]\n",
    "    wt_y = weight.shape[3]\n",
    "    c = weight.shape[1]\n",
    "    filter_count = weight.shape[0]\n",
    "    w = int((in_x+2*padding[0]-wt_x)//stride[0]+1)\n",
    "    h = int((in_y+2*padding[1]-wt_y)//stride[1]+1)\n",
    "    out = torch.empty(size=(1, filter_count, w, h))\n",
    "\n",
    "    mult_count = 0\n",
    "    calc_mult = 0\n",
    "    for i in range(filter_count):\n",
    "        comp_wt = comp_vector_pred(weight[i])\n",
    "        out[0][i], num1, num2 =compute_filter_conv_sparsepred(input, weight[i], comp_wt, w, h,stride, padding, bias[i])\n",
    "        # out[0][i] += bias[i]\n",
    "        mult_count+=num1\n",
    "        calc_mult+=num2\n",
    "    # mult_count is predictive sparse(weight only)\n",
    "    # calc_mult is baseline 2(sparse non-predictive)\n",
    "#     print(out)\n",
    "    return (out,mult_count)\n",
    "###################################################################################\n",
    "\n",
    "# ALGO 5\n",
    "class comp_vector_pred_twosided():\n",
    "  def __init__(self, arr):\n",
    "    self.x = arr.size(dim=2)\n",
    "    self.y = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.pos_vector = [] #stores tuples of (data, index)\n",
    "    self.neg_vector = []\n",
    "\n",
    "    for k in range(self.c):\n",
    "            for i in range(self.y):\n",
    "                for j in range(self.x):\n",
    "                    w = arr[k][i][j]\n",
    "                    if w > 0:\n",
    "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
    "                    elif w<0:\n",
    "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
    "\n",
    "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
    "\n",
    "  def get_pos_vector(self):\n",
    "    return self.pos_vector\n",
    "\n",
    "  def get_neg_vector(self):\n",
    "    return self.neg_vector\n",
    "\n",
    "\n",
    "def compute_conv_sparsepred_twosided(input, weight, comp_wt, r, c,bias=0):\n",
    "  img_out_cell = 0\n",
    "  conv_mult_count = 0\n",
    "  pos = comp_wt.get_pos_vector()\n",
    "  neg = comp_wt.get_neg_vector()\n",
    "\n",
    "  x = weight.shape[1]\n",
    "  y = weight.shape[2]\n",
    "  k = weight.shape[0]\n",
    "\n",
    "  mult_nonzero = 0\n",
    "  for channel in range(k):\n",
    "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
    "    inp_nonzero = np.flatnonzero(inp_window)\n",
    "    wt_nonzero = np.flatnonzero(weight[channel])\n",
    "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
    "    mult_nonzero += common\n",
    "\n",
    "  for tup in pos:\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "      continue\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  img_out_cell+=bias\n",
    "\n",
    "  # idx = 0\n",
    "  # while img_out_cell>=0 and idx<len(neg):\n",
    "  #   tup = neg[idx]\n",
    "  #   if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "  #     continue\n",
    "  #   conv_mult_count += 1\n",
    "  #   img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "  #   idx+=1\n",
    "\n",
    "  for tup in neg:\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "      continue\n",
    "    if img_out_cell < 0:\n",
    "      break\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  return img_out_cell, conv_mult_count, mult_nonzero\n",
    "\n",
    "\n",
    "def compute_filter_conv_sparsepred_twosided(input, weights, comp_wt, width_out, height_out,stride=(1,1), padding=(0,0), bias=0):\n",
    "    img_out_channel = torch.zeros(width_out, height_out)\n",
    "    filter_mult_count = 0\n",
    "    filter_calc_mult = 0\n",
    "    # print(input.shape)\n",
    "    # print(weights.shape)\n",
    "    # print(len(stride))\n",
    "    # print(len(padding))\n",
    "    for r in range(0,input.shape[2]+2*padding[0]-weights.shape[1]+1,stride[0]):\n",
    "        for c in range(0,input.shape[3]+2*padding[1]-weights.shape[2]+1,stride[1]):\n",
    "            r_out = int(r/stride[0])\n",
    "            c_out = int(c/stride[1])\n",
    "            img_out_channel[r_out][c_out], mult_count, calc_mult = compute_conv_sparsepred_twosided(input, weights,comp_wt, r, c, bias)\n",
    "            # img_out_channel[r][c] += bias \n",
    "            # Bias added in compute_conv_sparsepred_twosided\n",
    "            filter_mult_count += mult_count\n",
    "            filter_calc_mult += calc_mult\n",
    "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
    "\n",
    "\n",
    "def myconv2d_sparse_pred_twosided(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "  in_x = input.shape[2]\n",
    "  in_y = input.shape[3]\n",
    "  wt_x = weight.shape[2]\n",
    "  wt_y = weight.shape[3]\n",
    "  c = weight.shape[1]\n",
    "  filter_count = weight.shape[0]\n",
    "  w = int((in_x+2*padding[0]-wt_x)//stride[0]+1)\n",
    "  h = int((in_y+2*padding[1]-wt_y)//stride[1]+1)\n",
    "  # print(w,h)\n",
    "  out = torch.empty(size=(1, filter_count, w, h))\n",
    "\n",
    "  mult_count = 0\n",
    "  calc_mult = 0\n",
    "  for i in range(filter_count):\n",
    "    comp_wt = comp_vector_pred_twosided(weight[i])\n",
    "    out[0][i], num1, num2 =compute_filter_conv_sparsepred_twosided(input, weight[i], comp_wt, w, h,stride=stride,padding=padding,bias=bias[i])\n",
    "    mult_count+=num1\n",
    "    calc_mult+=num2\n",
    "  \n",
    "  return (out,mult_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69097af",
   "metadata": {},
   "source": [
    "**Defining custom Conv2D Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2561f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Conv2d(torch.nn.modules.conv._ConvNd):\n",
    "    \"\"\"\n",
    "    Implements a standard convolution layer that can be used as a regular module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        stride = (stride, stride)\n",
    "        padding = (padding, padding)\n",
    "        dilation = (dilation, dilation)\n",
    "        super(Custom_Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, (0, 0), groups, bias, padding_mode)\n",
    "\n",
    "    def conv2d_forward(self, input, weight):\n",
    "        return myconv2d_sparse_pred_twosided(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv2d_forward(input, self.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c02974",
   "metadata": {},
   "source": [
    "**Defining custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d1bb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty arrays for storing activation sparsities\n",
    "c1 = []\n",
    "c2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65eb6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty arrays for storing #MACops per layer\n",
    "m1 = []\n",
    "m2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2e29044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomLeNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            Custom_Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            Custom_Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(400, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print('Sparsity of CONV1 activations: ', (1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "        c1.append((1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "#         print(x.size())\n",
    "        out, macops1 = self.features[0](x) \n",
    "#         print(out.size())\n",
    "        m1.append(macops1)\n",
    "        movingtime = 0\n",
    "        s = time.time()\n",
    "        out = out.to(device)\n",
    "        movingtime += time.time()-s\n",
    "        out = self.features[1](out)\n",
    "        out = self.features[2](out)\n",
    "        out = self.features[3](out)\n",
    "        \n",
    "        # print('Sparsity of CONV2 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c2.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "#         print(out.size())\n",
    "        out, macops2 = self.features[4](out)\n",
    "#         print(out.size())\n",
    "        m2.append(macops2)\n",
    "        s = time.time()\n",
    "        out = out.to(device)\n",
    "        movingtime += time.time()-s\n",
    "#         print('movingtime: ', movingtime)\n",
    "        out = self.features[5](out)\n",
    "        out = self.features[6](out)\n",
    "        out = self.features[7](out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b087cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Instantiating a custom LeNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957a4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_lenet = CustomLeNet(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b2eab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Custom_Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Custom_Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "cust_lenet.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet_unnormalised.pth'))\n",
    "cust_lenet.to(device)\n",
    "cust_lenet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bda7b",
   "metadata": {},
   "source": [
    "# Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c553f3e",
   "metadata": {},
   "source": [
    "**Accuracy & Layer-wise Activation Sparsities(Dynamic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0237aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  m1.clear()\n",
    "  m2.clear()\n",
    "  c1.clear()\n",
    "  c2.clear()\n",
    "  with torch.no_grad():\n",
    "      for data in test_loader:\n",
    "          if(total>=100):\n",
    "            break\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))\n",
    "  print(\"CONV1 #MACops(avg):\", sum(m1)/len(m1))\n",
    "  print(\"CONV2 #MACops(avg):\", sum(m2)/len(m2))\n",
    "\n",
    "  print(\"CONV1 activation sparsity(avg):\", sum(c1)/len(c1))\n",
    "  print(\"CONV2 activation sparsity(avg):\", sum(c2)/len(c2))\n",
    "  # sum(c2)/len(c2)\n",
    "  print('number of test images: ', len(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b3b5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      for data in test_loader:\n",
    "            images1, labels = data[0].to(device), data[1].to(device)\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "561d617a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2118, 0.5843, 0.8157, 0.7373, 0.4941, 0.3216, 0.2118, 0.1843, 0.2196,\n",
       "         0.2078],\n",
       "        [0.5333, 0.8667, 0.7176, 0.3137, 0.1176, 0.0235, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.7098, 0.8745, 0.4745, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196,\n",
       "         0.1922],\n",
       "        [0.6706, 0.7686, 0.3647, 0.0039, 0.0000, 0.0000, 0.0000, 0.0392, 0.2314,\n",
       "         0.5961],\n",
       "        [0.5725, 0.7686, 0.4667, 0.1059, 0.0471, 0.0667, 0.1333, 0.2784, 0.7137,\n",
       "         0.9529],\n",
       "        [0.3765, 0.7765, 0.8314, 0.5529, 0.3020, 0.3922, 0.7294, 0.9216, 0.9882,\n",
       "         0.9961],\n",
       "        [0.0235, 0.4510, 0.8431, 0.9373, 0.9333, 0.8941, 0.9216, 0.7804, 0.9098,\n",
       "         0.9922],\n",
       "        [0.0000, 0.0784, 0.1882, 0.2902, 0.3412, 0.1725, 0.1765, 0.2706, 0.7608,\n",
       "         0.9255],\n",
       "        [0.0000, 0.0000, 0.0118, 0.0431, 0.0588, 0.0000, 0.0000, 0.4275, 0.8431,\n",
       "         0.8314],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6235, 0.9255,\n",
       "         0.6627]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images1[0][0][10:20, 10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9d47bd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          ...,\n",
      "          [-0.0570, -0.0577, -0.0594,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570]],\n",
      "\n",
      "         [[ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          ...,\n",
      "          [ 0.1148,  0.1133,  0.1037,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148]],\n",
      "\n",
      "         [[-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          ...,\n",
      "          [-0.4856, -0.4858, -0.4865,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856]],\n",
      "\n",
      "         [[ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          ...,\n",
      "          [ 0.0177,  0.0186,  0.0236,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177]],\n",
      "\n",
      "         [[-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          ...,\n",
      "          [-0.1714, -0.1700, -0.1641,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714]],\n",
      "\n",
      "         [[ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          ...,\n",
      "          [ 0.2500,  0.2501,  0.2508,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500]]]])\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#         out = myconv2d(images1, cust_lenet.features[0].weight, bias=cust_lenet.features[0].bias)[0]\n",
    "#         out = cust_lenet.features[1](out)\n",
    "#         out = cust_lenet.features[2](out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ee7b82c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          ...,\n",
      "          [-0.0570, -0.0577, -0.0594,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570],\n",
      "          [-0.0570, -0.0570, -0.0570,  ..., -0.0570, -0.0570, -0.0570]],\n",
      "\n",
      "         [[ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          ...,\n",
      "          [ 0.1148,  0.1133,  0.1123,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148],\n",
      "          [ 0.1148,  0.1148,  0.1148,  ...,  0.1148,  0.1148,  0.1148]],\n",
      "\n",
      "         [[-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          ...,\n",
      "          [-0.4856, -0.4858, -0.4865,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856],\n",
      "          [-0.4856, -0.4856, -0.4856,  ..., -0.4856, -0.4856, -0.4856]],\n",
      "\n",
      "         [[ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          ...,\n",
      "          [ 0.0177,  0.0186,  0.0236,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177],\n",
      "          [ 0.0177,  0.0177,  0.0177,  ...,  0.0177,  0.0177,  0.0177]],\n",
      "\n",
      "         [[-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          ...,\n",
      "          [-0.1714, -0.1700, -0.1641,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714],\n",
      "          [-0.1714, -0.1714, -0.1714,  ..., -0.1714, -0.1714, -0.1714]],\n",
      "\n",
      "         [[ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          ...,\n",
      "          [ 0.2500,  0.2501,  0.2508,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500],\n",
      "          [ 0.2500,  0.2500,  0.2500,  ...,  0.2500,  0.2500,  0.2500]]]])\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#         out = myconv2d_sparse_pred(images1, cust_lenet.features[0].weight, bias=cust_lenet.features[0].bias)[0]\n",
    "#         print(cust_lenet.features[1](out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "820ff425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "kernel_id 0\n",
      "kernel_id 0\n",
      "kernel_id 8\n",
      "Accuracy of the network on the test images: 60.00 %\n",
      "CONV1 #MACops(avg): 107257.8\n",
      "CONV2 #MACops(avg): 204219.7\n",
      "CONV1 activation sparsity(avg): 0.74087890625\n",
      "CONV2 activation sparsity(avg): 0.3985884314775467\n",
      "number of test images:  100\n",
      "1021.5858018398285\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(cust_lenet)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3332a",
   "metadata": {},
   "source": [
    "**Weight Sparsity (static)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lenet_w_sparsities(model):\n",
    "  conv_indices = [0, 4]\n",
    "\n",
    "  for i in range(2):\n",
    "    layer_index = conv_indices[i]\n",
    "\n",
    "    print(\n",
    "        \"Sparsity in conv{:}.weight: {:.2f}%\".format(i+1, \n",
    "            100. * float(torch.sum(model.features[layer_index].weight == 0))\n",
    "            / float(model.features[layer_index].weight.nelement())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1688a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 0.00%\n",
      "Sparsity in conv2.weight: 0.00%\n"
     ]
    }
   ],
   "source": [
    "get_lenet_w_sparsities(cust_lenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10708e4",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab5e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df7bd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_25 = CustomLeNet(10)\n",
    "lenet_25.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet.pth'))\n",
    "\n",
    "for name, module in lenet_25.named_modules():\n",
    "    # prune 25% of connections in all 2D-conv layers\n",
    "    if isinstance(module, Custom_Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a107f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 90.00%\n",
      "Sparsity in conv2.weight: 90.00%\n"
     ]
    }
   ],
   "source": [
    "get_lenet_w_sparsities(lenet_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e25a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 9.58 %\n",
      "CONV1 #MACops(avg): 117600.0\n",
      "CONV2 #MACops(avg): 240000.0\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.4483547618865967\n",
      "number of test images:  5000\n"
     ]
    }
   ],
   "source": [
    "test_model(lenet_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "450d963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLeNet(10)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet_unnormalised.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[4], 'weight')\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56f4a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 0.00%\n",
      "Sparsity in conv2.weight: 0.00%\n"
     ]
    }
   ],
   "source": [
    "get_lenet_w_sparsities(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3765c94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 91.00 %\n",
      "CONV1 #MACops(avg): 74432.44\n",
      "CONV2 #MACops(avg): 106843.69\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.6182653045654297\n",
      "number of test images:  100\n",
      "883.6397612094879\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(model)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c39a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLeNet(10)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet_unnormalised.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[4], 'weight')\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b191796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 87.00 %\n",
      "CONV1 #MACops(avg): 61475.91\n",
      "CONV2 #MACops(avg): 82405.74\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.6241241455078125\n",
      "number of test images:  100\n",
      "885.2152101993561\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(model)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17df6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLeNet(10)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet_unnormalised.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[4], 'weight')\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c92f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 87.00 %\n",
      "CONV1 #MACops(avg): 50100.01\n",
      "CONV2 #MACops(avg): 52344.15\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.64349489569664\n",
      "number of test images:  100\n",
      "570.5057721138\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(model)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "394fe7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLeNet(10)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet_unnormalised.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[4], 'weight')\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.75,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4125aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 79.00 %\n",
      "CONV1 #MACops(avg): 39107.71\n",
      "CONV2 #MACops(avg): 24672.83\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.6399064642190934\n",
      "number of test images:  100\n",
      "365.70060563087463\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(model)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcae27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a93aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
