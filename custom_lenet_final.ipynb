{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdb11f7",
   "metadata": {},
   "source": [
    "This notebook has custom AlexNet. It measures:\n",
    "\n",
    "\n",
    "1.   Sparsity of weights(one-time)\n",
    "2.   Layerwise CONV layer activation sparsities\n",
    "3.   Accuracy of the model\n",
    "4. Layerwise #MAC ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303bdf7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6a1b5596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f0ae0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "#                                                   transforms.Normalize(mean = (0.1325,), std = (0.3105,))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38f287",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fee6fa",
   "metadata": {},
   "source": [
    "**Custom conv2d function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8b4c5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALGO 1\n",
    "def myconv2d(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    \"\"\"\n",
    "    Function to process an input with a standard convolution\n",
    "    \"\"\"\n",
    "    mul_count = 0\n",
    "#     print('input', input.shape)\n",
    "#     print('wt', weight.shape)\n",
    "    batch_size, in_channels, in_h, in_w = input.shape\n",
    "    out_channels, in_channels, kh, kw = weight.shape\n",
    "    out_h = int((in_h - kh + 2 * padding[0]) / stride[0] + 1)\n",
    "    out_w = int((in_w - kw + 2 * padding[1]) / stride[1] + 1)\n",
    "    unfold = torch.nn.Unfold(kernel_size=(kh, kw), dilation=dilation, padding=padding, stride=stride)\n",
    "    inp_unf = unfold(input)\n",
    "    w_ = weight.view(weight.size(0), -1).t()\n",
    "    if bias is None:\n",
    "        out_unf = inp_unf.transpose(1, 2).matmul(w_).transpose(1, 2)\n",
    "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
    "    else:\n",
    "        out_unf = (inp_unf.transpose(1, 2).matmul(w_) + bias).transpose(1, 2)\n",
    "        mul_count += batch_size*out_channels*out_h*out_w*in_channels*kh*kw\n",
    "    out = out_unf.view(batch_size, out_channels, out_h, out_w)\n",
    "#     print(out)\n",
    "    return (out.float(), mul_count)\n",
    "    # return out.float()\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "# ALGO 2\n",
    "class comp_vector():\n",
    "  def __init__(self, arr):\n",
    "    self.x = arr.size(dim=2)\n",
    "    self.y = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.index_vector = []\n",
    "    self.data_vector = []\n",
    "    for i in range(self.c):\n",
    "      # print(arr[i])\n",
    "      self.index_vector.append(np.flatnonzero(arr[i].cpu()))\n",
    "      self.data_vector.append(arr[i].ravel()[self.index_vector[-1]])\n",
    "\n",
    "    # index_vector = np.flatnonzero(arr)\n",
    "    # data_vector = arr.ravel()[index_vector]\n",
    "\n",
    "  def get_index_vector(self):\n",
    "    return self.index_vector\n",
    "\n",
    "  def get_data_vector(self):\n",
    "    return self.data_vector\n",
    "\n",
    "\n",
    "def conv_compressed(comp_inp, comp_wt, stride=1):\n",
    "#     print('called conv_compressed')\n",
    "    acc_x, acc_y, acc_c = int((comp_inp.x - comp_wt.x)//stride  + 1) , int((comp_inp.y - comp_wt.y)//stride  +1), comp_wt.c\n",
    "#     print(acc_x, acc_y, acc_c)\n",
    "    mult_count = 0\n",
    "    # print(acc_x, acc_y, acc_c)\n",
    "    acc_buf = torch.FloatTensor(acc_x, acc_y).zero_()\n",
    "    inp_index_vector = comp_inp.get_index_vector()\n",
    "    inp_data_vector = comp_inp.get_data_vector()\n",
    "    wt_index_vector = comp_wt.get_index_vector()\n",
    "    wt_data_vector = comp_wt.get_data_vector()\n",
    "    # print(inp_index_vector[0])\n",
    "    # print(len(inp_index_vector[0]))\n",
    "    for c in range(acc_c):\n",
    "      for i in range(len(inp_index_vector[c])):\n",
    "        for j in range(len(wt_index_vector[c])):\n",
    "          inp_x = inp_index_vector[c][i]//comp_inp.x\n",
    "          inp_y = inp_index_vector[c][i]%comp_inp.y\n",
    "          wt_x = wt_index_vector[c][j]//comp_wt.x\n",
    "          wt_y = wt_index_vector[c][j]%comp_wt.y\n",
    "\n",
    "          out_x = (inp_x - wt_x)\n",
    "          out_y = (inp_y- wt_y)\n",
    "          if out_x%stride==0 and out_y%stride==0:\n",
    "            out_x = out_x//stride\n",
    "            out_y = out_y//stride\n",
    "            # print(out_x, out_y,c,i,j,)\n",
    "            if 0<=out_x<acc_x and 0<=out_y<acc_y:\n",
    "              # print(\"yes\")\n",
    "              acc_buf[out_x][out_y]+=float(inp_data_vector[c][i] * wt_data_vector[c][j])\n",
    "              mult_count +=1\n",
    "    \n",
    "    return (acc_buf,mult_count)\n",
    "\n",
    "def myconv2d_sparse(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "#   print(input.size())\n",
    "  comp_in = comp_vector(input[0])\n",
    "  in_x = input.size(dim=3)\n",
    "  in_y = input.size(dim=2)\n",
    "  wt_x = weight.size(dim=3)\n",
    "  wt_y = weight.size(dim=2)\n",
    "  c = weight.size(dim=1)\n",
    "  k = weight.size(dim=0)\n",
    "  out = torch.empty(size=(1,k, int((in_x-wt_x)/stride[0]+1), int((in_y-wt_y)/stride[1]+1)))\n",
    "\n",
    "  mult_count = 0\n",
    "  for i in range(k):\n",
    "    comp_wt = comp_vector(weight[i])\n",
    "    out[0][i], num =conv_compressed(comp_in, comp_wt, stride[0])\n",
    "    out[0][i] += bias[i]\n",
    "    mult_count+=num\n",
    "#   print(out)\n",
    "  return (out,mult_count)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "# ALGO 3\n",
    "def compute_weight_list(kernel):    \n",
    "    kernels = []\n",
    "    filter_count = kernel.shape[0]\n",
    "    depth = kernel.shape[1]\n",
    "    height = kernel.shape[2]\n",
    "    width = kernel.shape[3]\n",
    "    for f in range(filter_count):\n",
    "        weight_list = []\n",
    "        for k in range(depth):\n",
    "            for i in range(height):\n",
    "                for j in range(width):\n",
    "                    w = kernel[f][k][i][j]\n",
    "                    if w < 0:\n",
    "                        weight_list.append(tuple((w, k, i, j)))\n",
    "        sorted_weight_list = sorted(weight_list, key = lambda x: x[0])\n",
    "        kernels.append(sorted_weight_list)\n",
    "    return kernels\n",
    "\n",
    "def compute_conv_onlypred(img, weight_list, weights, r, c):\n",
    "    img_out_cell = 0\n",
    "    conv_mult_count = 0\n",
    "    depth = weights.shape[0]\n",
    "    height = weights.shape[1]\n",
    "    width = weights.shape[2]\n",
    "    for k in range(depth):\n",
    "        for i in range(width):\n",
    "            for j in range(height):          \n",
    "                if weights[k][i][j] > 0:\n",
    "                    conv_mult_count += 1 \n",
    "                    img_out_cell += img[0][k][r+i][c+j]*weights[k][i][j]\n",
    "    for tup in weight_list:\n",
    "        conv_mult_count += 1\n",
    "        img_out_cell += tup[0]*img[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "        if img_out_cell < 0:\n",
    "            break\n",
    "    return img_out_cell, conv_mult_count\n",
    "\n",
    "def compute_filter_conv_onlypred(img, weight_list, weights, kernel_id,stride=(1,1), padding=(0,0), bias=None):\n",
    "    width_out = int((img.shape[3]+2*padding[1]-weights.shape[2])/stride[1]+1)\n",
    "    height_out = int((img.shape[2]+2*padding[0]-weights.shape[1])/stride[0]+1)\n",
    "    img_out_channel = torch.zeros(width_out,height_out)\n",
    "    filter_mult_count = 0\n",
    "    # print(img.shape[2]+2*padding[0]-weights.shape[1], img.shape[3]+2*padding[1]-weights.shape[2])\n",
    "    for r in range(0,img.shape[2]+2*padding[0]-weights.shape[1]+1,stride[0]):\n",
    "        for c in range(0,img.shape[3]+2*padding[1]-weights.shape[2]+1,stride[1]):\n",
    "            r_out = int(r/stride[0])\n",
    "            c_out = int(c/stride[1])\n",
    "            # print(r_out, c_out)\n",
    "            img_out_channel[r_out][c_out], mult_count = compute_conv_onlypred(img, weight_list, weights, r, c)\n",
    "            # img_out_channel[r_out][c_out] += bias\n",
    "            filter_mult_count += mult_count\n",
    "    return img_out_channel, filter_mult_count\n",
    "\n",
    "def compute_conv_layer_onlypred(img, weights, bias=None, stride=(1,1), padding=(0,0)):\n",
    "    layer_mult_count = 0\n",
    "    filter_count = weights.shape[0]\n",
    "    depth = weights.shape[1]\n",
    "    height = weights.shape[2]\n",
    "    width = weights.shape[3]\n",
    "    channels_out=filter_count\n",
    "    width_out = int((img.shape[3]+2*padding[1]-width)/stride[1]+1)\n",
    "    height_out = int((img.shape[2]+2*padding[0]-height)/stride[0]+1)\n",
    "    img_conv_output = torch.zeros(channels_out, width_out, height_out)\n",
    "    filters_list = compute_weight_list(weights)\n",
    "    for kernel_id in range(filter_count):\n",
    "        if kernel_id%8==0:\n",
    "            print(\"kernel_id\", kernel_id)\n",
    "        weight_list = filters_list[kernel_id]\n",
    "        img_conv_channel, mult_count = compute_filter_conv_onlypred(img, weight_list, weights[kernel_id], kernel_id, stride, padding)\n",
    "        img_conv_output[kernel_id] = img_conv_channel\n",
    "        layer_mult_count += mult_count\n",
    "    return img_conv_output, layer_mult_count\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# ALGO 4\n",
    "class comp_vector_pred():\n",
    "  def __init__(self, arr):\n",
    "    self.y = arr.size(dim=2)\n",
    "    self.x = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.pos_vector = [] #stores tuples of (data, index)\n",
    "    self.neg_vector = []\n",
    "\n",
    "    for k in range(self.c):\n",
    "            for i in range(self.x):\n",
    "                for j in range(self.y):\n",
    "                    w = arr[k][i][j]\n",
    "                    if w > 0:\n",
    "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
    "                    elif w<0:\n",
    "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
    "\n",
    "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
    "\n",
    "  def get_pos_vector(self):\n",
    "    return self.pos_vector\n",
    "\n",
    "  def get_neg_vector(self):\n",
    "    return self.neg_vector\n",
    "\n",
    "\n",
    "def compute_conv(input, weight, comp_wt, r, c):\n",
    "  img_out_cell = 0\n",
    "  conv_mult_count = 0\n",
    "  pos = comp_wt.get_pos_vector()\n",
    "  neg = comp_wt.get_neg_vector()\n",
    "\n",
    "  x = weight.shape[1]\n",
    "  y = weight.shape[2]\n",
    "  k = weight.shape[0]\n",
    "\n",
    "  mult_nonzero = 0\n",
    "  for channel in range(k):\n",
    "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
    "    inp_nonzero = np.flatnonzero(inp_window)\n",
    "    wt_nonzero = np.flatnonzero(weight[channel])\n",
    "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
    "    mult_nonzero += common\n",
    "\n",
    "  for tup in pos:\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "        continue\n",
    "    conv_mult_count += 1\n",
    "#     if c+tup[3]>=231:\n",
    "#         print(c, tup[3])\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "  for tup in neg:\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "        continue\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "    if img_out_cell < 0:\n",
    "      break\n",
    "\n",
    "  return img_out_cell, conv_mult_count, mult_nonzero\n",
    "\n",
    "\n",
    "def compute_filter_conv(input, weights, comp_wt, width_out, height_out):\n",
    "#     print('called compute_filter_conv')\n",
    "    img_out_channel = torch.zeros(width_out, height_out)\n",
    "    filter_mult_count = 0\n",
    "    filter_calc_mult = 0\n",
    "    for r in range(0,width_out):\n",
    "        for c in range(0,height_out):\n",
    "            img_out_channel[r][c], mult_count, calc_mult = compute_conv(input, weights,comp_wt, r, c)\n",
    "            # img_out_channel[r][c] += bias\n",
    "            filter_mult_count += mult_count\n",
    "            filter_calc_mult += calc_mult\n",
    "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
    "\n",
    "\n",
    "def myconv2d_sparse_pred(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "    input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "    in_x = input.shape[2]\n",
    "    in_y = input.shape[3]\n",
    "    wt_x = weight.shape[2]\n",
    "    wt_y = weight.shape[3]\n",
    "    c = weight.shape[1]\n",
    "    filter_count = weight.shape[0]\n",
    "    w = int((in_x-wt_x)//stride[0]+1)\n",
    "    h = int((in_y-wt_y)//stride[1]+1)\n",
    "#     print(in_x, padding[0], wt_x, stride[0])\n",
    "#     print(in_y, padding[1], wt_y, stride[1])\n",
    "#     print(w,h)\n",
    "    out = torch.empty(size=(1, filter_count, w, h))\n",
    "\n",
    "    mult_count = 0\n",
    "    calc_mult = 0\n",
    "    for i in range(filter_count):\n",
    "        comp_wt = comp_vector_pred(weight[i])\n",
    "        out[0][i], num1, num2 =compute_filter_conv(input, weight[i], comp_wt, w, h)\n",
    "        out[0][i] += bias[i]\n",
    "        mult_count+=num1\n",
    "        calc_mult+=num2\n",
    "    # mult_count is predictive sparse(weight only)\n",
    "    # calc_mult is baseline 2(sparse non-predictive)\n",
    "#     print(out)\n",
    "    return (out,mult_count)\n",
    "###################################################################################\n",
    "\n",
    "# ALGO 5\n",
    "class comp_vector_pred():\n",
    "  def __init__(self, arr):\n",
    "    self.x = arr.size(dim=2)\n",
    "    self.y = arr.size(dim=1)\n",
    "    self.c = arr.size(dim=0)\n",
    "    self.pos_vector = [] #stores tuples of (data, index)\n",
    "    self.neg_vector = []\n",
    "\n",
    "    for k in range(self.c):\n",
    "            for i in range(self.y):\n",
    "                for j in range(self.x):\n",
    "                    w = arr[k][i][j]\n",
    "                    if w > 0:\n",
    "                      self.pos_vector.append(tuple((w, k, i, j)))\n",
    "                    elif w<0:\n",
    "                      self.neg_vector.append(tuple((w, k, i, j)))\n",
    "\n",
    "    self.neg_vector = sorted(self.neg_vector, key = lambda x: x[0])\n",
    "\n",
    "  def get_pos_vector(self):\n",
    "    return self.pos_vector\n",
    "\n",
    "  def get_neg_vector(self):\n",
    "    return self.neg_vector\n",
    "\n",
    "\n",
    "def compute_conv_sparsepred(input, weight, comp_wt, r, c):\n",
    "  img_out_cell = 0\n",
    "  conv_mult_count = 0\n",
    "  pos = comp_wt.get_pos_vector()\n",
    "  neg = comp_wt.get_neg_vector()\n",
    "\n",
    "  x = weight.shape[1]\n",
    "  y = weight.shape[2]\n",
    "  k = weight.shape[0]\n",
    "\n",
    "  mult_nonzero = 0\n",
    "  for channel in range(k):\n",
    "    inp_window = input[0][channel][r:r+x, c:c+y]\n",
    "    inp_nonzero = np.flatnonzero(inp_window)\n",
    "    wt_nonzero = np.flatnonzero(weight[channel])\n",
    "    common = sum(X == Y for X, Y in zip(inp_nonzero, wt_nonzero))\n",
    "    mult_nonzero += common\n",
    "\n",
    "  for tup in pos:\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "      continue\n",
    "    conv_mult_count += 1\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "\n",
    "\n",
    "  idx = 0\n",
    "  while img_out_cell>=0 and idx<len(neg):\n",
    "    if(input[0][tup[1]][r+tup[2]][c+tup[3]]==0):\n",
    "      continue\n",
    "    conv_mult_count += 1\n",
    "    # print(neg, idx)\n",
    "    # print(idx)\n",
    "    tup = neg[idx]\n",
    "    img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "    idx+=1\n",
    "\n",
    "  # for tup in neg:\n",
    "  #   conv_mult_count += 1\n",
    "  #   img_out_cell += tup[0]*input[0][tup[1]][r+tup[2]][c+tup[3]]\n",
    "  #   if img_out_cell < 0:\n",
    "  #     break\n",
    "\n",
    "  return img_out_cell, conv_mult_count, mult_nonzero\n",
    "\n",
    "\n",
    "def compute_filter_conv_sparsepred(input, weights, comp_wt, width_out, height_out):\n",
    "    img_out_channel = torch.zeros(width_out, height_out)\n",
    "    filter_mult_count = 0\n",
    "    filter_calc_mult = 0\n",
    "    for r in range(0,width_out):\n",
    "        for c in range(0,height_out):\n",
    "            img_out_channel[r][c], mult_count, calc_mult = compute_conv_sparsepred(input, weights,comp_wt, r, c)\n",
    "            # img_out_channel[r][c] += bias\n",
    "            filter_mult_count += mult_count\n",
    "            filter_calc_mult += calc_mult\n",
    "    return img_out_channel, filter_mult_count, filter_calc_mult\n",
    "\n",
    "\n",
    "def myconv2d_sparse_pred(input, weight, bias=None, stride=(1,1), padding=(0,0), dilation=(1,1), groups=1):\n",
    "  input = torch.nn.functional.pad(input, (padding[1], padding[1], padding[0], padding[0]), \"constant\", 0)\n",
    "  in_x = input.shape[2]\n",
    "  in_y = input.shape[3]\n",
    "  wt_x = weight.shape[2]\n",
    "  wt_y = weight.shape[3]\n",
    "  c = weight.shape[1]\n",
    "  filter_count = weight.shape[0]\n",
    "  w = int((in_x-wt_x)//stride[0]+1)\n",
    "  h = int((in_y-wt_y)//stride[1]+1)\n",
    "  print(w,h)\n",
    "  out = torch.empty(size=(1, filter_count, w, h))\n",
    "\n",
    "  mult_count = 0\n",
    "  calc_mult = 0\n",
    "  for i in range(filter_count):\n",
    "    comp_wt = comp_vector_pred(weight[i])\n",
    "    out[0][i], num1, num2 =compute_filter_conv_sparsepred(input, weight[i], comp_wt, w, h)\n",
    "    mult_count+=num1\n",
    "    calc_mult+=num2\n",
    "  \n",
    "  return (out,mult_count, calc_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69097af",
   "metadata": {},
   "source": [
    "**Defining custom Conv2D Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2561f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Conv2d(torch.nn.modules.conv._ConvNd):\n",
    "    \"\"\"\n",
    "    Implements a standard convolution layer that can be used as a regular module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        stride = (stride, stride)\n",
    "        padding = (padding, padding)\n",
    "        dilation = (dilation, dilation)\n",
    "        super(Custom_Conv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, (0, 0), groups, bias, padding_mode)\n",
    "\n",
    "    def conv2d_forward(self, input, weight):\n",
    "        return myconv2d_sparse(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv2d_forward(input, self.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c02974",
   "metadata": {},
   "source": [
    "**Defining custom AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "9d1bb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty arrays for storing activation sparsities\n",
    "c1 = []\n",
    "c2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "65eb6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty arrays for storing #MACops per layer\n",
    "m1 = []\n",
    "m2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "f2e29044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomLeNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            Custom_Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            Custom_Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(400, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print('Sparsity of CONV1 activations: ', (1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "        c1.append((1 - torch.count_nonzero(x)/torch.numel(x)).item())\n",
    "#         print(x.size())\n",
    "        out, macops1 = self.features[0](x) \n",
    "#         print(out.size())\n",
    "        m1.append(macops1)\n",
    "        movingtime = 0\n",
    "        s = time.time()\n",
    "        out = out.to(device)\n",
    "        movingtime += time.time()-s\n",
    "        out = self.features[1](out)\n",
    "        out = self.features[2](out)\n",
    "        out = self.features[3](out)\n",
    "        \n",
    "        # print('Sparsity of CONV2 activations: ', (1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "        c2.append((1 - torch.count_nonzero(out)/torch.numel(out)).item())\n",
    "#         print(out.size())\n",
    "        out, macops2 = self.features[4](out)\n",
    "#         print(out.size())\n",
    "        m2.append(macops2)\n",
    "        s = time.time()\n",
    "        out = out.to(device)\n",
    "        movingtime += time.time()-s\n",
    "#         print('movingtime: ', movingtime)\n",
    "        out = self.features[5](out)\n",
    "        out = self.features[6](out)\n",
    "        out = self.features[7](out)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b087cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Instantiating a custom LeNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "957a4c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_lenet = CustomLeNet(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "6b2eab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Custom_Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Custom_Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "cust_lenet.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet_unnormalised.pth'))\n",
    "cust_lenet.to(device)\n",
    "cust_lenet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75bda7b",
   "metadata": {},
   "source": [
    "# Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c553f3e",
   "metadata": {},
   "source": [
    "**Accuracy & Layer-wise Activation Sparsities(Dynamic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "0237aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  m1.clear()\n",
    "  m2.clear()\n",
    "  c1.clear()\n",
    "  c2.clear()\n",
    "  with torch.no_grad():\n",
    "      for data in test_loader:\n",
    "          if(total>=1000):\n",
    "            break\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy of the network on the test images: %.2f %%' % (100 * correct / total))\n",
    "  print(\"CONV1 #MACops(avg):\", sum(m1)/len(m1))\n",
    "  print(\"CONV2 #MACops(avg):\", sum(m2)/len(m2))\n",
    "\n",
    "  print(\"CONV1 activation sparsity(avg):\", sum(c1)/len(c1))\n",
    "  print(\"CONV2 activation sparsity(avg):\", sum(c2)/len(c2))\n",
    "  # sum(c2)/len(c2)\n",
    "  print('number of test images: ', len(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "50333676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "      for data in test_loader:\n",
    "            images1, labels = data[0].to(device), data[1].to(device)\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "db7fc8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0471, 0.0980, 0.2784, 0.5569,\n",
       "         0.8510],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0314, 0.2000, 0.4510, 0.6588, 0.8431,\n",
       "         0.9529],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0510, 0.2471, 0.6314, 0.8784, 0.9765, 0.9922,\n",
       "         0.9922],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1216, 0.5020, 0.9882, 0.9961, 0.9922, 0.9922,\n",
       "         0.9922],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1098, 0.4588, 0.9059, 0.9922, 0.9922, 0.9922,\n",
       "         0.9922],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0078, 0.1020, 0.5020, 0.9059, 0.9451, 0.9608,\n",
       "         0.9804],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0353, 0.0588, 0.2706, 0.4941,\n",
       "         0.8157],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0431, 0.1216,\n",
       "         0.5412],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118,\n",
       "         0.4039],\n",
       "        [0.0824, 0.1098, 0.0667, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392,\n",
       "         0.4667]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images1[0][0][10:20, 10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "9a08b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0126,  0.2092,  0.3881,  0.4867,  0.6075,  0.6791,  0.6056,  0.5112,\n",
      "          0.3985,  0.3063],\n",
      "        [-0.0651,  0.1434,  0.4028,  0.5484,  0.5665,  0.6068,  0.6490,  0.6164,\n",
      "          0.5150,  0.3529],\n",
      "        [-0.1758, -0.1493, -0.0289,  0.1208,  0.2979,  0.4895,  0.5811,  0.6126,\n",
      "          0.5926,  0.5378],\n",
      "        [-0.2714, -0.4309, -0.5533, -0.4512, -0.0220,  0.3476,  0.5124,  0.6034,\n",
      "          0.6477,  0.7172],\n",
      "        [-0.2660, -0.4834, -0.7189, -0.7859, -0.6254, -0.2176,  0.1996,  0.4683,\n",
      "          0.6694,  0.9117],\n",
      "        [-0.1777, -0.2605, -0.4761, -0.7687, -0.9042, -0.7146, -0.3151,  0.0656,\n",
      "          0.3670,  0.7060],\n",
      "        [-0.1356, -0.1272, -0.1385, -0.1886, -0.2716, -0.3445, -0.3042, -0.1456,\n",
      "          0.1009,  0.4553],\n",
      "        [-0.0156, -0.0376, -0.0134,  0.0084,  0.0264,  0.0100, -0.0397, -0.0264,\n",
      "          0.0977,  0.3394],\n",
      "        [ 0.3377,  0.2063,  0.1909,  0.2176,  0.2396,  0.2697,  0.2604,  0.2083,\n",
      "          0.1798,  0.2190],\n",
      "        [ 0.8911,  0.7054,  0.5297,  0.4518,  0.4855,  0.5504,  0.5527,  0.4100,\n",
      "          0.2274,  0.1840]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        out = myconv2d(images1, cust_lenet.features[0].weight, bias=cust_lenet.features[0].bias)[0]\n",
    "        print(out[0][0][10:20, 10:20])\n",
    "#         out = cust_lenet.features[1](out)\n",
    "#         out = cust_lenet.features[2](out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1369ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_id 0\n",
      "tensor([[ 1.4964e-01,  3.7143e-01,  5.5033e-01,  6.4901e-01,  7.6974e-01,\n",
      "          8.4140e-01,  7.6784e-01,  6.7345e-01,  5.6073e-01,  4.6854e-01],\n",
      "        [ 9.7123e-02,  3.0572e-01,  5.6512e-01,  7.1069e-01,  7.2876e-01,\n",
      "          7.6911e-01,  8.1131e-01,  7.7868e-01,  6.7729e-01,  5.1513e-01],\n",
      "        [-1.3525e-02,  1.2992e-02,  1.3335e-01,  2.8308e-01,  4.6013e-01,\n",
      "          6.5177e-01,  7.4341e-01,  7.7484e-01,  7.5484e-01,  7.0002e-01],\n",
      "        [-1.2537e-02, -7.8293e-02, -1.8739e-01, -6.1671e-02,  1.4027e-01,\n",
      "          5.0983e-01,  6.7467e-01,  7.6568e-01,  8.0997e-01,  8.7942e-01],\n",
      "        [-3.2210e-02, -2.2111e-02, -7.8940e-02, -9.6553e-02, -2.1579e-01,\n",
      "         -3.7414e-04,  3.6182e-01,  6.3054e-01,  8.3165e-01,  1.0740e+00],\n",
      "        [-5.3914e-03, -1.9191e-02, -2.3669e-02, -1.6954e-01, -2.5522e-01,\n",
      "         -1.7075e-02, -4.2152e-02,  2.2787e-01,  5.2930e-01,  8.6829e-01],\n",
      "        [ 2.6713e-02,  3.5116e-02,  2.3802e-02, -2.3574e-02, -2.9500e-02,\n",
      "         -2.6547e-02, -5.2825e-02,  1.6697e-02,  2.6322e-01,  6.1756e-01],\n",
      "        [ 1.4672e-01,  1.2471e-01,  1.4883e-01,  1.7069e-01,  1.8871e-01,\n",
      "          1.7222e-01,  1.2259e-01,  1.3590e-01,  2.5999e-01,  5.0165e-01],\n",
      "        [ 4.9997e-01,  3.6859e-01,  3.5315e-01,  3.7991e-01,  4.0186e-01,\n",
      "          4.3196e-01,  4.2271e-01,  3.7060e-01,  3.4209e-01,  3.8129e-01],\n",
      "        [ 1.0534e+00,  8.6765e-01,  6.9200e-01,  6.1411e-01,  6.4772e-01,\n",
      "          7.1262e-01,  7.1496e-01,  5.7227e-01,  3.8968e-01,  3.4625e-01]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        out2 = algo3(images1, cust_lenet.features[0].weight, bias=cust_lenet.features[0].bias)[0]\n",
    "        print(out2[0][10:20, 10:20])\n",
    "#         print(cust_lenet.features[1](out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ff9662f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25999999046325684"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 - torch.count_nonzero(out[0][0][10:20, 10:20]-out2[0][0][10:20, 10:20])/torch.numel(out[0][0][10:20, 10:20]-out2[0][0][10:20, 10:20])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f6890f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -1.4901e-08, -2.9802e-08,  0.0000e+00,  0.0000e+00,\n",
      "          1.1921e-07, -1.1921e-07, -8.9407e-08, -5.9605e-08, -5.9605e-08],\n",
      "        [-1.4901e-08, -1.4901e-08, -2.9802e-08, -2.9802e-08, -2.9802e-08,\n",
      "         -1.1921e-07, -5.9605e-08,  1.1921e-07,  0.0000e+00,  0.0000e+00],\n",
      "        [ 7.4506e-09, -1.4901e-08,  5.9605e-08, -4.4703e-08, -2.9802e-08,\n",
      "         -5.9605e-08,  8.9407e-08,  5.9605e-08, -8.9407e-08,  2.9802e-08],\n",
      "        [ 1.4901e-08, -2.9802e-08, -5.9605e-08,  1.0431e-07, -8.9407e-08,\n",
      "          0.0000e+00,  1.1921e-07, -2.9802e-08,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  5.9605e-08, -5.9605e-08, -5.9605e-08,  1.0431e-07,\n",
      "         -2.9802e-08, -1.1921e-07, -2.9802e-08, -2.9802e-08, -7.4506e-09],\n",
      "        [ 0.0000e+00, -5.9605e-08, -1.1921e-07,  2.0862e-07, -5.9605e-08,\n",
      "          8.9407e-08,  0.0000e+00,  5.9605e-08, -1.4901e-08,  0.0000e+00],\n",
      "        [ 0.0000e+00,  5.9605e-08,  2.9802e-08, -1.4901e-07,  1.4901e-07,\n",
      "          2.9802e-08, -5.9605e-08,  0.0000e+00,  7.4506e-09,  0.0000e+00],\n",
      "        [-8.9407e-08,  2.9802e-08,  8.9407e-08, -2.9802e-08,  5.9605e-08,\n",
      "          0.0000e+00,  2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-5.9605e-08,  8.9407e-08,  0.0000e+00, -5.9605e-08, -5.9605e-08,\n",
      "         -5.9605e-08,  0.0000e+00,  7.4506e-09,  0.0000e+00,  0.0000e+00],\n",
      "        [ 2.9802e-08,  5.9605e-08,  2.9802e-08, -1.1921e-07,  5.9605e-08,\n",
      "          5.9605e-08, -2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(out[0][0][10:20, 10:20]-out2[0][0][10:20, 10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "820ff425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.00 %\n",
      "CONV1 #MACops(avg): 40008.6\n",
      "CONV2 #MACops(avg): 125821.76\n",
      "CONV1 activation sparsity(avg): 0.737392578125\n",
      "CONV2 activation sparsity(avg): 0.5703061193227768\n",
      "number of test images:  100\n",
      "491.5386610031128\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(cust_lenet)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3332a",
   "metadata": {},
   "source": [
    "**Weight Sparsity (static)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2d5435b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lenet_w_sparsities(model):\n",
    "  conv_indices = [0, 4]\n",
    "\n",
    "  for i in range(2):\n",
    "    layer_index = conv_indices[i]\n",
    "\n",
    "    print(\n",
    "        \"Sparsity in conv{:}.weight: {:.2f}%\".format(i+1, \n",
    "            100. * float(torch.sum(model.features[layer_index].weight == 0))\n",
    "            / float(model.features[layer_index].weight.nelement())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1688a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 0.00%\n",
      "Sparsity in conv2.weight: 0.00%\n"
     ]
    }
   ],
   "source": [
    "get_lenet_w_sparsities(cust_lenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10708e4",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "dab5e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "df7bd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_25 = CustomLeNet(10)\n",
    "lenet_25.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet.pth'))\n",
    "\n",
    "for name, module in lenet_25.named_modules():\n",
    "    # prune 25% of connections in all 2D-conv layers\n",
    "    if isinstance(module, Custom_Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "a107f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 90.00%\n",
      "Sparsity in conv2.weight: 90.00%\n"
     ]
    }
   ],
   "source": [
    "get_lenet_w_sparsities(lenet_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e25a452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 9.58 %\n",
      "CONV1 #MACops(avg): 117600.0\n",
      "CONV2 #MACops(avg): 240000.0\n",
      "CONV1 activation sparsity(avg): 0.0\n",
      "CONV2 activation sparsity(avg): 0.4483547618865967\n",
      "number of test images:  5000\n"
     ]
    }
   ],
   "source": [
    "test_model(lenet_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "450d963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLeNet(10)\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\rjsha\\Downloads\\lenet_unnormalised.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.features[0], 'weight'),\n",
    "    (model.features[4], 'weight')\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.75,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "56f4a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in conv1.weight: 50.00%\n",
      "Sparsity in conv2.weight: 76.54%\n"
     ]
    }
   ],
   "source": [
    "get_lenet_w_sparsities(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3765c94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 88.70 %\n",
      "CONV1 #MACops(avg): 19290.836\n",
      "CONV2 #MACops(avg): 29985.286\n",
      "CONV1 activation sparsity(avg): 0.7465078125\n",
      "CONV2 activation sparsity(avg): 0.5762517013549805\n",
      "number of test images:  1000\n",
      "1754.514051914215\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "test_model(model)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39a89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
